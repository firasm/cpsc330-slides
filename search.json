[
  {
    "objectID": "slides-03.html#announcements",
    "href": "slides-03.html#announcements",
    "title": "Lecture 3: ML fundamentals",
    "section": "Announcements",
    "text": "Announcements\n\nMy weekly office hours will be announced soon!\nHomework 2 (hw2) was released on Monday, it is due Jan 19, 11:59 pm\n\nYou are welcome to broadly discuss it with your classmates but final answers and submissions must be your own.\nGroup submissions are not allowed for this assignment.\n\nAdvice on keeping up with the material\n\nPractice!\nMake sure you run the lecture notes on your laptop and experiment with the code.\nStart early on homework assignments.\n\nLast day to drop without a W standing is this Friday: January 16, 2026"
  },
  {
    "objectID": "slides-03.html#dropping-lowest-homework-update",
    "href": "slides-03.html#dropping-lowest-homework-update",
    "title": "Lecture 3: ML fundamentals",
    "section": "Dropping lowest homework (Update)",
    "text": "Dropping lowest homework (Update)\n\nCPSC 330 has 9 homework assignments that are all an integral part of your learning\nTo account for illnesses, other commitments, and to preserve your mental health, there has long been a policy of dropping your lowest HW score.\nAfter some analysis from the data from previous terms (Learning Analytics!), there is a slight modification to this policy:\n\n\nWith the exception of HW5, we will drop your lowest homework grade - all students must complete HW5.\n\n\nThis is to encourage all students to complete HW5! It‚Äôs important!"
  },
  {
    "objectID": "slides-03.html#recap",
    "href": "slides-03.html#recap",
    "title": "Lecture 3: ML fundamentals",
    "section": "Recap",
    "text": "Recap\n\nImportance of generalization in supervised machine learning\nData splitting as a way to approximate generalization error\nTrain, test, validation, deployment data\nOverfitting, underfitting, the fundamental tradeoff, and the golden rule.\nCross-validation"
  },
  {
    "objectID": "slides-03.html#finish-up-demo-from-last-class",
    "href": "slides-03.html#finish-up-demo-from-last-class",
    "title": "Lecture 3: ML fundamentals",
    "section": "Finish up demo from last class",
    "text": "Finish up demo from last class\nFor this demo, each student should click this link to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today."
  },
  {
    "objectID": "slides-03.html#recap-1",
    "href": "slides-03.html#recap-1",
    "title": "Lecture 3: ML fundamentals",
    "section": "Recap",
    "text": "Recap\nA typical sequence of steps to train supervised machine learning models\n\ntraining the model on the train split\ntuning hyperparamters using the validation split\nchecking the generalization performance on the test split"
  },
  {
    "objectID": "slides-03.html#clicker-3.1",
    "href": "slides-03.html#clicker-3.1",
    "title": "Lecture 3: ML fundamentals",
    "section": "Clicker 3.1",
    "text": "Clicker 3.1\nParticipate using Agora (code: agentic)\nSelect all of the following statements which are TRUE.\n\n\nA decision tree model with no depth (the default max_depth in sklearn) is likely to perform very well on the deployment data.\n\n\nData splitting helps us assess how well our model would generalize.\n\n\nDeployment data is scored only once.\n\n\nValidation data could be used for hyperparameter optimization.\n\n\nIt‚Äôs recommended that data be shuffled before splitting it into train and test sets."
  },
  {
    "objectID": "slides-03.html#clicker-3.2",
    "href": "slides-03.html#clicker-3.2",
    "title": "Lecture 3: ML fundamentals",
    "section": "Clicker 3.2",
    "text": "Clicker 3.2\nParticipate using Agora (code: agentic)\nSelect all of the following statements which are TRUE.\n\n\n\\(k\\)-fold cross-validation calls fit \\(k\\) times\n\n\nWe use cross-validation to get a more robust estimate of model performance.\n\n\nIf the mean train accuracy is much higher than the mean cross-validation accuracy it‚Äôs likely to be a case of overfitting.\n\n\nThe fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n\n\nA decision stump on a complicated classification problem is likely to underfit."
  },
  {
    "objectID": "slides-03.html#recap-from-videos",
    "href": "slides-03.html#recap-from-videos",
    "title": "Lecture 3: ML fundamentals",
    "section": "Recap from videos",
    "text": "Recap from videos\n\nWhy do we split the data? What are train/valid/test splits?\nWhat are the benefits of cross-validation?\nWhat is underfitting and overfitting?\nWhat‚Äôs the fundamental trade-off in supervised machine learning?\nWhat is the golden rule of machine learning?"
  },
  {
    "objectID": "slides-03.html#summary-of-train-validation-test-and-deployment-data",
    "href": "slides-03.html#summary-of-train-validation-test-and-deployment-data",
    "title": "Lecture 3: ML fundamentals",
    "section": "Summary of train, validation, test, and deployment data",
    "text": "Summary of train, validation, test, and deployment data\n\n\n\n\nfit\nscore\npredict\n\n\n\n\nTrain\n‚úîÔ∏è\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nValidation\n\n‚úîÔ∏è\n‚úîÔ∏è\n\n\nTest\n\nonce\nonce\n\n\nDeployment\n\n\n‚úîÔ∏è"
  },
  {
    "objectID": "slides-03.html#cross-validation",
    "href": "slides-03.html#cross-validation",
    "title": "Lecture 3: ML fundamentals",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides-03.html#cross-validation-1",
    "href": "slides-03.html#cross-validation-1",
    "title": "Lecture 3: ML fundamentals",
    "section": "Cross validation",
    "text": "Cross validation"
  },
  {
    "objectID": "slides-03.html#overfitting-and-underfitting",
    "href": "slides-03.html#overfitting-and-underfitting",
    "title": "Lecture 3: ML fundamentals",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\n\n\nSource\n\n\nAn overfit model matches the training set so closely that it fails to make correct predictions on new unseen data.\n\nAn underfit model is too simple and does not even make good predictions on the training data"
  },
  {
    "objectID": "slides-03.html#the-fundamental-tradeoff",
    "href": "slides-03.html#the-fundamental-tradeoff",
    "title": "Lecture 3: ML fundamentals",
    "section": "The fundamental tradeoff",
    "text": "The fundamental tradeoff\nAs you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.\n\n\n\n\n\n\n\n\n\nUnderfitting: Both accuracies rise\nSweet spot: Validation accuracy peaks\nOverfitting: Training \\(\\uparrow\\), Validation \\(\\downarrow\\)\nTradeoff: Balance complexity to avoid both"
  },
  {
    "objectID": "slides-03.html#the-golden-rule",
    "href": "slides-03.html#the-golden-rule",
    "title": "Lecture 3: ML fundamentals",
    "section": "The golden rule",
    "text": "The golden rule\n\nAlthough our primary concern is the model‚Äôs performance on the test data, this data should not influence the training process in any way.\n\n\n\n Source: Image generated by ChatGPT 5\n\n\nTest data = final exam\n\nYou can practice all you want with training/validation data\nBut never peek at the test set before evaluation\nOtherwise, it‚Äôs like sneaking answers before the exam \\(\\rightarrow\\) not a real assessment of your learning."
  },
  {
    "objectID": "slides-03.html#additional-resource-on-cross-validation",
    "href": "slides-03.html#additional-resource-on-cross-validation",
    "title": "Lecture 3: ML fundamentals",
    "section": "Additional Resource on Cross Validation",
    "text": "Additional Resource on Cross Validation\n\n\n\n        \n        \n\n\nReference: MLU-Explain - Cross Validation"
  },
  {
    "objectID": "slides-03.html#break",
    "href": "slides-03.html#break",
    "title": "Lecture 3: ML fundamentals",
    "section": "Break",
    "text": "Break\nLet‚Äôs take a break!"
  },
  {
    "objectID": "slides-03.html#group-work-class-demo-live-coding",
    "href": "slides-03.html#group-work-class-demo-live-coding",
    "title": "Lecture 3: ML fundamentals",
    "section": "Group Work: Class Demo & Live Coding",
    "text": "Group Work: Class Demo & Live Coding\nFor this demo, each student should click this link to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today."
  },
  {
    "objectID": "slides-01.html#introductions",
    "href": "slides-01.html#introductions",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ü§ù Introductions ! ü§ù",
    "text": "ü§ù Introductions ! ü§ù"
  },
  {
    "objectID": "slides-01.html#about-your-instructor",
    "href": "slides-01.html#about-your-instructor",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "About your instructor",
    "text": "About your instructor"
  },
  {
    "objectID": "slides-01.html#about-my-research-interests",
    "href": "slides-01.html#about-my-research-interests",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "About my research interests",
    "text": "About my research interests"
  },
  {
    "objectID": "slides-01.html#group-work-in-this-class",
    "href": "slides-01.html#group-work-in-this-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Group work in this class",
    "text": "Group work in this class\nThis term we will try to work in ‚ÄúPods‚Äù of 3-5 ‚Ä¶\nResearch shows that there is tremendous benefits in students working (and struggling) together!\nStudents ask better and more insightful questions, engage more deeply with the work, and it adds a social element to class.\nWe will try this in CPSC 330 this term!"
  },
  {
    "objectID": "slides-01.html#group-work-in-this-class-1",
    "href": "slides-01.html#group-work-in-this-class-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Group work in this class",
    "text": "Group work in this class\nUnderstandably, not everyone is a fan of group work - I understand that!\nSo you will never be forced to work in groups ‚Äã‚ÄãIf you would like to opt-out, move to the far left and far right sides of the room so we know you prefer to work individually.\nIf everyone moves to the side of the room, we will re-evaluate this approach üòÇ\nThere are no marks or points associated with these groups, and everyone should work on their own laptops as well"
  },
  {
    "objectID": "slides-01.html#group-work-pods",
    "href": "slides-01.html#group-work-pods",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Group work: Pods",
    "text": "Group work: Pods\nForm a Pod of 3-5 people sitting close to you.\nEach person should answer the following questions:\n\nPreferred Name,\nYear,\n(intended) Major\nWhy are you taking CPSC 330?\n\nThen, as a group, answer the following question:\nWhat is the most interesting (good or bad) example of Machine Learning in society?"
  },
  {
    "objectID": "slides-01.html#meet-eva-a-fictitious-persona",
    "href": "slides-01.html#meet-eva-a-fictitious-persona",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Meet Eva (a fictitious persona)!",
    "text": "Meet Eva (a fictitious persona)!\n\n\n\n\nEva is among one of you. She has some experience in Python programming. She knows machine learning as a buzz word. During her recent internship, she has developed some interest and curiosity in the field. She wants to learn what is it and how to use it. She is a curious person and usually has a lot of questions!"
  },
  {
    "objectID": "slides-01.html#learning-outcomes",
    "href": "slides-01.html#learning-outcomes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "üéØ Learning Outcomes",
    "text": "üéØ Learning Outcomes\nBy the end of this lesson, you will be able to:\n\nExplain the difference between AI, ML, and DL\nDescribe what machine learning is and when it is appropriate to use ML-based solutions.\nBriefly describe supervised learning.\nDifferentiate between traditional programming and machine learning.\nEvaluate whether a machine learning solution is suitable for your problem or whether a rule-based or human-expert solution is more appropriate.\nNavigate the course materials and get familiar with the course syllabus and policies."
  },
  {
    "objectID": "slides-01.html#about-this-course",
    "href": "slides-01.html#about-this-course",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "About this course",
    "text": "About this course"
  },
  {
    "objectID": "slides-01.html#cpsc-330-website",
    "href": "slides-01.html#cpsc-330-website",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 website",
    "text": "CPSC 330 website\n\nCourse Jupyter book: https://ubc-cs.github.io/cpsc330-2025W2\nCourse GitHub repository: https://github.com/UBC-CS/cpsc330-2025W2\n\n\n\n\n\n\n\nImportant\n\n\nCourse website: https://ubc-cs.github.io/cpsc330-2025W2 is the most important link. You can access the course website from Canvas.\n\nPlease read everything on there!\nYou can find the source code for everything we do here: https://ubc-cs.github.io/cpsc330-2025W2.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMake sure you go through the syllabus thoroughly and complete the syllabus quiz before next class!"
  },
  {
    "objectID": "slides-01.html#asking-questions-during-class",
    "href": "slides-01.html#asking-questions-during-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Asking questions during class",
    "text": "Asking questions during class\nYou are welcome to ask questions by raising your hand!\nIf you would prefer to write notes and ask questions later, you are more than welcome to do that also! Use Ed Discussion."
  },
  {
    "objectID": "slides-01.html#registration-waitlist-and-prerequisites",
    "href": "slides-01.html#registration-waitlist-and-prerequisites",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Registration, waitlist and prerequisites",
    "text": "Registration, waitlist and prerequisites\n\n\n\n\n\n\nImportant\n\n\nPlease go through this document carefully before contacting your instructors about these issues. Even then, we are very unlikely to be able to help with registration, waitlist or prerequisite issues.\n\n\n\n\nWe are expecting that all students registered on the waitlist have already, or will soon get a notification to join the course!\nThe waitlist will close on Friday at 3 PM and no more students will be able to register after that!\nIt is your responsibility to catch up on any missed work."
  },
  {
    "objectID": "slides-01.html#lecture-format",
    "href": "slides-01.html#lecture-format",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture format",
    "text": "Lecture format\n\nIn person lectures Tuesday and Thursday from 3:30 PM - 5 PM\nThere will be videos to watch before almost every lecture. You will find the list of pre-watch videos in the schedule on the course webpage.\nWe will also try to work on some questions and exercises together during the class.\nAll materials will be posted in this GitHub repository.  \n\nYou may attend any tutorials or office hours your want, regardless of in which/whether you‚Äôre registered."
  },
  {
    "objectID": "slides-01.html#home-work-assignments",
    "href": "slides-01.html#home-work-assignments",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Home work assignments",
    "text": "Home work assignments\n\nFirst homework assignment is due soon, it will be released to you today.\nThis is a relatively straightforward assignment on Python. If you struggle with this assignment then that could be a sign that you will struggle later on in the course.\n\nYou must do the first two homework assignments on your own."
  },
  {
    "objectID": "slides-01.html#exams",
    "href": "slides-01.html#exams",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Exams",
    "text": "Exams\n\nWe‚Äôll have two self-scheduled midterms over a few day window and one final exam in Computer-based Testing Facility (CBTF)."
  },
  {
    "objectID": "slides-01.html#course-structure",
    "href": "slides-01.html#course-structure",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course structure",
    "text": "Course structure\n\nPart I: Introduction, ML fundamentals, preprocessing, midterm 1\nPart II: Unsupervised learning, transfer learning, common special cases, midterm 1\nPart III: Communication and ethics\n\nML skills are not beneficial if you can‚Äôt use them responsibly and communicate your results. In this module we‚Äôll talk about these aspects. ## Code of conduct\n\nOur main forum for getting help will be Ed Discussion.\n\n\n\n\n\n\n\nImportant\n\n\nPlease read this entire document about asking for help. TLDR: Be nice."
  },
  {
    "objectID": "slides-01.html#course-conda-environment",
    "href": "slides-01.html#course-conda-environment",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course conda environment",
    "text": "Course conda environment\n\nFollow the setup instructions here to create a course conda environment on your computer.\nIf you do not have your computer with you, you can partner up with someone and set up your own computer later."
  },
  {
    "objectID": "slides-01.html#python-requirementsresources",
    "href": "slides-01.html#python-requirementsresources",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Python requirements/resources",
    "text": "Python requirements/resources\nWe will primarily use Python in this course.\nHere is the basic Python knowledge you‚Äôll need for the course:\n\nBasic Python programming\nNumpy\nPandas\nBasic matplotlib\n\nHomework 1 is all about Python.\n\n\n\n\n\n\nNote\n\n\nWe do not have time to teach all the Python we need but you can find some useful Python resources here."
  },
  {
    "objectID": "slides-01.html#workload",
    "href": "slides-01.html#workload",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Workload",
    "text": "Workload\nWhat does a typical week look like?\n\nBefore class: Watch pre-lecture videos or preview notes\n\nIn class: Two 80-minute lectures with iClicker questions, activities, and live demos\n\nSupport: Weekly tutorials and office hours\nPractice: Weekly assignments (except exam weeks)"
  },
  {
    "objectID": "slides-01.html#tips-for-success",
    "href": "slides-01.html#tips-for-success",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Tips for success:",
    "text": "Tips for success:\n\nAttend lectures regularly and ask questions\n\nStart homework early. Hands-on practice is essential\nUse Generative AI tools responsibly. No blind copy-pasting\nAlways question your data, methods, and results ‚Äî justify your choices"
  },
  {
    "objectID": "slides-01.html#homework-format-jupyter-lab-notebooks",
    "href": "slides-01.html#homework-format-jupyter-lab-notebooks",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Homework format: Jupyter lab notebooks",
    "text": "Homework format: Jupyter lab notebooks\n\nOur notes are created in a Jupyter notebook, with file extension .ipynb.\nAlso, you will complete your homework assignments using Jupyter notebooks.\nConfusingly, ‚ÄúJupyter notebook‚Äù is also the original application that opens .ipynb files - but has since been replaced by Jupyter lab.\n\nI am using Jupyter lab, some things might not work with the Jupyter notebook application.\nYou can also open these files in Visual Studio Code."
  },
  {
    "objectID": "slides-01.html#jupyter-lab-notebooks",
    "href": "slides-01.html#jupyter-lab-notebooks",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Jupyter lab notebooks",
    "text": "Jupyter lab notebooks\n\nNotebooks contain a mix of code, code output, markdown-formatted text (including LaTeX equations), and more.\nWhen you open a Jupyter notebook in one of these apps, the document is ‚Äúlive‚Äù, meaning you can run the code.\n\nFor example:\n\n1 + 1\n\n2\n\n\n\nx = [1, 2, 3]\nx[0] = 9999\nx\n\n[9999, 2, 3]"
  },
  {
    "objectID": "slides-01.html#more-about-jupyter-lab",
    "href": "slides-01.html#more-about-jupyter-lab",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "More about Jupyter lab",
    "text": "More about Jupyter lab\n\nBy default, Jupyter prints out the result of the last line of code, so you don‚Äôt need as many print statements.\nIn addition to the ‚Äúlive‚Äù notebooks, Jupyter notebooks can be statically rendered in the web browser, e.g.¬†this.\n\nThis can be convenient for quick read-only access, without needing to launch the Jupyter notebook/lab application.\nBut you need to launch the app properly to interact with the notebooks."
  },
  {
    "objectID": "slides-01.html#lecture-notes",
    "href": "slides-01.html#lecture-notes",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Lecture notes",
    "text": "Lecture notes\n\nAll the lectures from last year are available here.\nWe cannot promise anything will stay the same from last year to this year, so read them in advance at your own risk.\nA ‚Äúfinalized‚Äù version will be pushed to GitHub and the Jupyter book right before each class.\nEach instructor will have slightly adapted versions of notes to present slides during lectures.\n\nYou will find the link to these slides in our repository: https://github.com/UBC-CS/cpsc330-2025W2/tree/main/lectures/103-Firas-lectures"
  },
  {
    "objectID": "slides-01.html#grades",
    "href": "slides-01.html#grades",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Grades",
    "text": "Grades\n\nThe grading breakdown is here.\nThe policy on challenging grades is here."
  },
  {
    "objectID": "slides-01.html#setting-up-your-computer-for-the-course-1",
    "href": "slides-01.html#setting-up-your-computer-for-the-course-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Setting up your computer for the course",
    "text": "Setting up your computer for the course"
  },
  {
    "objectID": "slides-01.html#recommended-browser-and-tools",
    "href": "slides-01.html#recommended-browser-and-tools",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Recommended browser and tools",
    "text": "Recommended browser and tools\n\nYou can install Chrome here.\nYou can install Firefox here.\n\nIn this course, we will primarily be using Python , git, GitHub, Canvas, Gradescope, Ed DIscussion, and PrairieLearn."
  },
  {
    "objectID": "slides-01.html#course-conda-environment-1",
    "href": "slides-01.html#course-conda-environment-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Course conda environment",
    "text": "Course conda environment\n\nFollow the setup instructions here to create a course conda environment on your computer.\nIf you do not have your computer with you, you can partner up with someone and set up your own computer later."
  },
  {
    "objectID": "slides-01.html#python-requirementsresources-1",
    "href": "slides-01.html#python-requirementsresources-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Python requirements/resources",
    "text": "Python requirements/resources\nWe will primarily use Python in this course.\nHere is the basic Python knowledge you‚Äôll need for the course:\n\nBasic Python programming\nNumpy\nPandas\nBasic matplotlib\nSparse matrices\n\nHomework 1 is all about Python.\n\n\n\n\n\n\nNote\n\n\nWe do not have time to teach all the Python we need but you can find some useful Python resources here."
  },
  {
    "objectID": "slides-01.html#cpsc-330-vs.-340",
    "href": "slides-01.html#cpsc-330-vs.-340",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "CPSC 330 vs.¬†340",
    "text": "CPSC 330 vs.¬†340\nRead https://ubc-cs.github.io/cpsc330-2025W2/docs/330_vs_340.html which explains the difference between two courses.\nTLDR:\n\n340: how do ML models work?\n330: how do I use ML models?\nCPSC 340 has many prerequisites.\nCPSC 340 goes deeper but has a more narrow scope.\nI think CPSC 330 will be more useful if you just plan to apply basic ML."
  },
  {
    "objectID": "slides-01.html#break",
    "href": "slides-01.html#break",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Break",
    "text": "Break"
  },
  {
    "objectID": "slides-01.html#activity-1",
    "href": "slides-01.html#activity-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity 1",
    "text": "Activity 1\n\nDiscuss with you neighbour\n\nWhat do you know about machine learning?\nWhat would you like to get out this course?\nAre there any particular topics or aspects of this course that you are especially excited or anxious about? Why?"
  },
  {
    "objectID": "slides-01.html#which-cat-do-you-think-is-ai-generated",
    "href": "slides-01.html#which-cat-do-you-think-is-ai-generated",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Which cat do you think is AI-generated?",
    "text": "Which cat do you think is AI-generated?\n\n\n\nSource\n\n\nA\nB\nBoth\nNone\n\n\n\nWhat clues did you use to decide?"
  },
  {
    "objectID": "slides-01.html#what-are-ai-ml-dl",
    "href": "slides-01.html#what-are-ai-ml-dl",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What are AI, ML, DL?",
    "text": "What are AI, ML, DL?\n\nArtificial Intelligence (AI): Making computers act smart\n\nExamples: Deep Blue, early spell checkers\n\nMachine Learning (ML): Learning patterns from data\n\nExample: Spam filtering in Gmail\n\nDeep Learning (DL): Using neural networks to learn complex patterns\n\nExamples: Face recognition in your phone, voice assistants"
  },
  {
    "objectID": "slides-01.html#lets-walk-through-an-example",
    "href": "slides-01.html#lets-walk-through-an-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Let‚Äôs walk through an example",
    "text": "Let‚Äôs walk through an example\n\nHave you used search in Google Photos? You can search for ‚Äúcat‚Äù and it will retrieve photos from your libraries containing cats.\nThis can be done using image classification."
  },
  {
    "objectID": "slides-01.html#image-classification",
    "href": "slides-01.html#image-classification",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Image classification",
    "text": "Image classification\n\nImagine we want a system that can tell cats and foxes apart.\nHow might we do this with traditional programming? With ML?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage ID\nWhiskers Present\nEar Size\nFace Shape\nFur Color\nEye Shape\nLabel\n\n\n\n\n1\nYes\nLarge\nRound\nMixed\nRound\nCat\n\n\n2\nYes\nMedium\nRound\nBrown\nAlmond\nCat\n\n\n3\nYes\nLarge\nPointed\nRed\nNarrow\nFox\n\n\n4\nYes\nLarge\nPointed\nRed\nNarrow\nFox\n\n\n5\nYes\nSmall\nRound\nMixed\nRound\nCat\n\n\n6\nYes\nLarge\nPointed\nRed\nNarrow\nFox\n\n\n7\nYes\nSmall\nRound\nGrey\nRound\nCat\n\n\n8\nYes\nSmall\nRound\nBlack\nRound\nCat\n\n\n9\nYes\nLarge\nPointed\nRed\nNarrow\nFox"
  },
  {
    "objectID": "slides-01.html#traditional-programming-example",
    "href": "slides-01.html#traditional-programming-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Traditional programming: example",
    "text": "Traditional programming: example\n\n\n\n\n\nYou hard-code rules. If all of the following satisfy, it‚Äôs a fox.\n\npointed face ‚úÖ\nred fur ‚úÖ\nnarrow eyes ‚úÖ\n\n\nThis works for normal cases, but what if there are exceptions"
  },
  {
    "objectID": "slides-01.html#ml-approach-example",
    "href": "slides-01.html#ml-approach-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ML approach: example",
    "text": "ML approach: example\n\n\n\n\n\nWe don‚Äôt tell the model the exact rule. Instead, we give it many labeled images, and it learns probabilistic patterns across multiple features, not rigid rules.\n\nIf fur is red \\(\\rightarrow\\) 90% chance of Fox."
  },
  {
    "objectID": "slides-01.html#dl-approach-example",
    "href": "slides-01.html#dl-approach-example",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "DL approach: example",
    "text": "DL approach: example\n\n\n\n\n\nA neural network automatically learns which features to look at (edges \\(\\rightarrow\\) textures \\(\\rightarrow\\) objects).\nNo need to even specify face shape or fur colour. It learns relevant features on its own."
  },
  {
    "objectID": "slides-01.html#what-is-ml",
    "href": "slides-01.html#what-is-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is ML?",
    "text": "What is ML?\n\nML uses algorithms to learn patterns from data and build models.\n\nThese models can:\n\nMake predictions on new data\n\nSupport complex decisions\n\nGenerate new content\n\n\nML systems can improve when trained on more data.\n\nThere is no one-size-fits-all model. The right choice depends on the problem."
  },
  {
    "objectID": "slides-01.html#when-to-use-ml",
    "href": "slides-01.html#when-to-use-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "When to use ML?",
    "text": "When to use ML?\n\nWhen the problem can‚Äôt be solved with a fixed set of rules\nWhen you have lots of data and complex relationships\nWhen human decision-making is too slow or inconsistent\n\n\n\n\n\n\n\n\nApproach\nBest for\n\n\n\n\nTraditional Programming\nRules are known, data is clean/predictable\n\n\nMachine Learning\nRules are complex/unknown, data is noisy"
  },
  {
    "objectID": "slides-01.html#example-supervised-classification",
    "href": "slides-01.html#example-supervised-classification",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Supervised classification",
    "text": "Example: Supervised classification\n\nWe want to predict liver disease from tabular features:\n\n\n\n\n\n\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\nTarget\n\n\n\n\n40\n14.5\n6.4\n358\n50\n75\n5.7\n2.1\n0.50\nDisease\n\n\n33\n0.7\n0.2\n256\n21\n30\n8.5\n3.9\n0.80\nDisease\n\n\n24\n0.7\n0.2\n188\n11\n10\n5.5\n2.3\n0.71\nNo Disease\n\n\n60\n0.7\n0.2\n171\n31\n26\n7.0\n3.5\n1.00\nNo Disease\n\n\n18\n0.8\n0.2\n199\n34\n31\n6.5\n3.5\n1.16\nNo Disease"
  },
  {
    "objectID": "slides-01.html#model-training",
    "href": "slides-01.html#model-training",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Model training",
    "text": "Model training\n\nfrom lightgbm.sklearn import LGBMClassifier\nmodel = LGBMClassifier(random_state=123, verbose=-1)\nmodel.fit(X_train, y_train)\n\nLGBMClassifier(random_state=123, verbose=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nboosting_type\n'gbdt'\n\n\n\nnum_leaves\n31\n\n\n\nmax_depth\n-1\n\n\n\nlearning_rate\n0.1\n\n\n\nn_estimators\n100\n\n\n\nsubsample_for_bin\n200000\n\n\n\nobjective\nNone\n\n\n\nclass_weight\nNone\n\n\n\nmin_split_gain\n0.0\n\n\n\nmin_child_weight\n0.001\n\n\n\nmin_child_samples\n20\n\n\n\nsubsample\n1.0\n\n\n\nsubsample_freq\n0\n\n\n\ncolsample_bytree\n1.0\n\n\n\nreg_alpha\n0.0\n\n\n\nreg_lambda\n0.0\n\n\n\nrandom_state\n123\n\n\n\nn_jobs\nNone\n\n\n\nimportance_type\n'split'\n\n\n\nverbose\n-1"
  },
  {
    "objectID": "slides-01.html#new-examples",
    "href": "slides-01.html#new-examples",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "New examples",
    "text": "New examples\n\nGiven features of new patients below we‚Äôll use this model to predict whether these patients have the liver disease or not.\n\n\n\n\n\n\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\n\n\n\n\n19\n1.4\n0.8\n178\n13\n26\n8.0\n4.6\n1.30\n\n\n12\n1.0\n0.2\n719\n157\n108\n7.2\n3.7\n1.00\n\n\n60\n5.7\n2.8\n214\n412\n850\n7.3\n3.2\n0.78\n\n\n42\n0.5\n0.1\n162\n155\n108\n8.1\n4.0\n0.90"
  },
  {
    "objectID": "slides-01.html#model-predictions-on-new-examples",
    "href": "slides-01.html#model-predictions-on-new-examples",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Model predictions on new examples",
    "text": "Model predictions on new examples\n\nLet‚Äôs examine predictions\n\n\npred_df = pd.DataFrame({\"Predicted_target\": model.predict(X_test).tolist()})\ndf_concat = pd.concat([pred_df, X_test.reset_index(drop=True)], axis=1)\nHTML(df_concat.to_html(index=False))\n\n\n\n\nPredicted_target\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\n\n\n\n\nNo Disease\n19\n1.4\n0.8\n178\n13\n26\n8.0\n4.6\n1.30\n\n\nDisease\n12\n1.0\n0.2\n719\n157\n108\n7.2\n3.7\n1.00\n\n\nDisease\n60\n5.7\n2.8\n214\n412\n850\n7.3\n3.2\n0.78\n\n\nDisease\n42\n0.5\n0.1\n162\n155\n108\n8.1\n4.0\n0.90"
  },
  {
    "objectID": "slides-01.html#example-supervised-regression",
    "href": "slides-01.html#example-supervised-regression",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Supervised regression",
    "text": "Example: Supervised regression\nSuppose we want to predict housing prices given a number of attributes associated with houses. The target here is continuous and not discrete.\n\n\n\n\n\ntarget\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n509000.0\n2\n1.50\n1930\n3521\n2.0\n0\n0\n3\n8\n1930\n0\n1989\n0\n98007\n47.6092\n-122.146\n1840\n3576\n\n\n675000.0\n5\n2.75\n2570\n12906\n2.0\n0\n0\n3\n8\n2570\n0\n1987\n0\n98075\n47.5814\n-122.050\n2580\n12927\n\n\n420000.0\n3\n1.00\n1150\n5120\n1.0\n0\n0\n4\n6\n800\n350\n1946\n0\n98116\n47.5588\n-122.392\n1220\n5120\n\n\n680000.0\n8\n2.75\n2530\n4800\n2.0\n0\n0\n4\n7\n1390\n1140\n1901\n0\n98112\n47.6241\n-122.305\n1540\n4800\n\n\n357823.0\n3\n1.50\n1240\n9196\n1.0\n0\n0\n3\n8\n1240\n0\n1968\n0\n98072\n47.7562\n-122.094\n1690\n10800"
  },
  {
    "objectID": "slides-01.html#building-a-regression-model",
    "href": "slides-01.html#building-a-regression-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Building a regression model",
    "text": "Building a regression model\n\nfrom lightgbm.sklearn import LGBMRegressor\n\nX_train, y_train = train_df.drop(columns= [\"target\"]), train_df[\"target\"]\nX_test, y_test = test_df.drop(columns= [\"target\"]), train_df[\"target\"]\n\nmodel = LGBMRegressor()\nmodel.fit(X_train, y_train);"
  },
  {
    "objectID": "slides-01.html#predicting-prices-of-unseen-houses",
    "href": "slides-01.html#predicting-prices-of-unseen-houses",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting prices of unseen houses",
    "text": "Predicting prices of unseen houses\n\npred_df = pd.DataFrame(\n    {\"Predicted_target\": model.predict(X_test[0:4]).tolist()}\n)\ndf_concat = pd.concat([pred_df, X_test[0:4].reset_index(drop=True)], axis=1)\nHTML(df_concat.to_html(index=False))\n\n\n\n\nPredicted_target\nbedrooms\nbathrooms\nsqft_living\nsqft_lot\nfloors\nwaterfront\nview\ncondition\ngrade\nsqft_above\nsqft_basement\nyr_built\nyr_renovated\nzipcode\nlat\nlong\nsqft_living15\nsqft_lot15\n\n\n\n\n345831.740542\n4\n2.25\n2130\n8078\n1.0\n0\n0\n4\n7\n1380\n750\n1977\n0\n98055\n47.4482\n-122.209\n2300\n8112\n\n\n601042.018745\n3\n2.50\n2210\n7620\n2.0\n0\n0\n3\n8\n2210\n0\n1994\n0\n98052\n47.6938\n-122.130\n1920\n7440\n\n\n311310.186024\n4\n1.50\n1800\n9576\n1.0\n0\n0\n4\n7\n1800\n0\n1977\n0\n98045\n47.4664\n-121.747\n1370\n9576\n\n\n597555.592401\n3\n2.50\n1580\n1321\n2.0\n0\n2\n3\n8\n1080\n500\n2014\n0\n98107\n47.6688\n-122.402\n1530\n1357\n\n\n\n\n\nWe are predicting continuous values here as apposed to discrete values in disease vs.¬†no disease example."
  },
  {
    "objectID": "slides-01.html#example-text-classification",
    "href": "slides-01.html#example-text-classification",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Text classification",
    "text": "Example: Text classification\n\nSuppose you are given some data with labeled spam and non-spam messages and you want to predict whether a new message is spam or not spam.\n\n\nCodeOutput\n\n\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\n\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\nspam\nLookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\n\n\nham\nAight, I'll hit you up when I get some cash\n\n\nham\nDon no da:)whats you plan?\n\n\nham\nGoing to take your babe out ?\n\n\nham\nNo need lar. Jus testing e phone card. Dunno network not gd i thk. Me waiting 4 my sis 2 finish bathing so i can bathe. Dun disturb u liao u cleaning ur room."
  },
  {
    "objectID": "slides-01.html#lets-train-a-model",
    "href": "slides-01.html#lets-train-a-model",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Let‚Äôs train a model",
    "text": "Let‚Äôs train a model\n\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\nclf = make_pipeline(CountVectorizer(max_features=5000), LogisticRegression(max_iter=5000))\nclf.fit(X_train, y_train) # Training the model\n\nPipeline(steps=[('countvectorizer', CountVectorizer(max_features=5000)),\n                ('logisticregression', LogisticRegression(max_iter=5000))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nsteps steps: list of tuples\n\nList of (name of step, estimator) tuples that are to be chained in\nsequential order. To be compatible with the scikit-learn API, all steps\nmust define `fit`. All non-last steps must also define `transform`. See\n:ref:`Combining Estimators ` for more details.\n[('countvectorizer', ...), ('logisticregression', ...)]\n\n\n\ntransform_input transform_input: list of str, default=None\n\nThe names of the :term:`metadata` parameters that should be transformed by the\npipeline before passing it to the step consuming it.\n\nThis enables transforming some input arguments to ``fit`` (other than ``X``)\nto be transformed by the steps of the pipeline up to the step which requires\nthem. Requirement is defined via :ref:`metadata routing `.\nFor instance, this can be used to pass a validation set through the pipeline.\n\nYou can only set this if metadata routing is enabled, which you\ncan enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n.. versionadded:: 1.6\nNone\n\n\n\nmemory memory: str or object with the joblib.Memory interface, default=None\n\nUsed to cache the fitted transformers of the pipeline. The last step\nwill never be cached, even if it is a transformer. By default, no\ncaching is performed. If a string is given, it is the path to the\ncaching directory. Enabling caching triggers a clone of the transformers\nbefore fitting. Therefore, the transformer instance given to the\npipeline cannot be inspected directly. Use the attribute ``named_steps``\nor ``steps`` to inspect estimators within the pipeline. Caching the\ntransformers is advantageous when fitting is time consuming. See\n:ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\nfor an example on how to enable caching.\nNone\n\n\n\nverbose verbose: bool, default=False\n\nIf True, the time elapsed while fitting each step will be printed as it\nis completed.\nFalse\n\n\n\n\n            \n        \n    CountVectorizer?Documentation for CountVectorizer\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\ninput input: {'filename', 'file', 'content'}, default='content'\n\n- If `'filename'`, the sequence passed as an argument to fit is\nexpected to be a list of filenames that need reading to fetch\nthe raw content to analyze.\n\n- If `'file'`, the sequence items must have a 'read' method (file-like\nobject) that is called to fetch the bytes in memory.\n\n- If `'content'`, the input is expected to be a sequence of items that\ncan be of type string or byte.\n'content'\n\n\n\nencoding encoding: str, default='utf-8'\n\nIf bytes or files are given to analyze, this encoding is used to\ndecode.\n'utf-8'\n\n\n\ndecode_error decode_error: {'strict', 'ignore', 'replace'}, default='strict'\n\nInstruction on what to do if a byte sequence is given to analyze that\ncontains characters not of the given `encoding`. By default, it is\n'strict', meaning that a UnicodeDecodeError will be raised. Other\nvalues are 'ignore' and 'replace'.\n'strict'\n\n\n\nstrip_accents strip_accents: {'ascii', 'unicode'} or callable, default=None\n\nRemove accents and perform other character normalization\nduring the preprocessing step.\n'ascii' is a fast method that only works on characters that have\na direct ASCII mapping.\n'unicode' is a slightly slower method that works on any characters.\nNone (default) means no character normalization is performed.\n\nBoth 'ascii' and 'unicode' use NFKD normalization from\n:func:`unicodedata.normalize`.\nNone\n\n\n\nlowercase lowercase: bool, default=True\n\nConvert all characters to lowercase before tokenizing.\nTrue\n\n\n\npreprocessor preprocessor: callable, default=None\n\nOverride the preprocessing (strip_accents and lowercase) stage while\npreserving the tokenizing and n-grams generation steps.\nOnly applies if ``analyzer`` is not callable.\nNone\n\n\n\ntokenizer tokenizer: callable, default=None\n\nOverride the string tokenization step while preserving the\npreprocessing and n-grams generation steps.\nOnly applies if ``analyzer == 'word'``.\nNone\n\n\n\nstop_words stop_words: {'english'}, list, default=None\n\nIf 'english', a built-in stop word list for English is used.\nThere are several known issues with 'english' and you should\nconsider an alternative (see :ref:`stop_words`).\n\nIf a list, that list is assumed to contain stop words, all of which\nwill be removed from the resulting tokens.\nOnly applies if ``analyzer == 'word'``.\n\nIf None, no stop words will be used. In this case, setting `max_df`\nto a higher value, such as in the range (0.7, 1.0), can automatically detect\nand filter stop words based on intra corpus document frequency of terms.\nNone\n\n\n\ntoken_pattern token_pattern: str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n\nRegular expression denoting what constitutes a \"token\", only used\nif ``analyzer == 'word'``. The default regexp select tokens of 2\nor more alphanumeric characters (punctuation is completely ignored\nand always treated as a token separator).\n\nIf there is a capturing group in token_pattern then the\ncaptured group content, not the entire match, becomes the token.\nAt most one capturing group is permitted.\n'(?u)\\\\b\\\\w\\\\w+\\\\b'\n\n\n\nngram_range ngram_range: tuple (min_n, max_n), default=(1, 1)\n\nThe lower and upper boundary of the range of n-values for different\nword n-grams or char n-grams to be extracted. All values of n such\nsuch that min_n &lt;= n &lt;= max_n will be used. For example an\n``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\nunigrams and bigrams, and ``(2, 2)`` means only bigrams.\nOnly applies if ``analyzer`` is not callable.\n(1, ...)\n\n\n\nanalyzer analyzer: {'word', 'char', 'char_wb'} or callable, default='word'\n\nWhether the feature should be made of word n-gram or character\nn-grams.\nOption 'char_wb' creates character n-grams only from text inside\nword boundaries; n-grams at the edges of words are padded with space.\n\nIf a callable is passed it is used to extract the sequence of features\nout of the raw, unprocessed input.\n\n.. versionchanged:: 0.21\n\nSince v0.21, if ``input`` is ``filename`` or ``file``, the data is\nfirst read from the file and then passed to the given callable\nanalyzer.\n'word'\n\n\n\nmax_df max_df: float in range [0.0, 1.0] or int, default=1.0\n\nWhen building the vocabulary ignore terms that have a document\nfrequency strictly higher than the given threshold (corpus-specific\nstop words).\nIf float, the parameter represents a proportion of documents, integer\nabsolute counts.\nThis parameter is ignored if vocabulary is not None.\n1.0\n\n\n\nmin_df min_df: float in range [0.0, 1.0] or int, default=1\n\nWhen building the vocabulary ignore terms that have a document\nfrequency strictly lower than the given threshold. This value is also\ncalled cut-off in the literature.\nIf float, the parameter represents a proportion of documents, integer\nabsolute counts.\nThis parameter is ignored if vocabulary is not None.\n1\n\n\n\nmax_features max_features: int, default=None\n\nIf not None, build a vocabulary that only consider the top\n`max_features` ordered by term frequency across the corpus.\nOtherwise, all features are used.\n\nThis parameter is ignored if vocabulary is not None.\n5000\n\n\n\nvocabulary vocabulary: Mapping or iterable, default=None\n\nEither a Mapping (e.g., a dict) where keys are terms and values are\nindices in the feature matrix, or an iterable over terms. If not\ngiven, a vocabulary is determined from the input documents. Indices\nin the mapping should not be repeated and should not have any gap\nbetween 0 and the largest index.\nNone\n\n\n\nbinary binary: bool, default=False\n\nIf True, all non zero counts are set to 1. This is useful for discrete\nprobabilistic models that model binary events rather than integer\ncounts.\nFalse\n\n\n\ndtype dtype: dtype, default=np.int64\n\nType of the matrix returned by fit_transform() or transform().\n&lt;class 'numpy.int64'&gt;\n\n\n\n\n            \n        \n    LogisticRegression?Documentation for LogisticRegression\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\npenalty penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'\n\nSpecify the norm of the penalty:\n\n- `None`: no penalty is added;\n- `'l2'`: add a L2 penalty term and it is the default choice;\n- `'l1'`: add a L1 penalty term;\n- `'elasticnet'`: both L1 and L2 penalty terms are added.\n\n.. warning::\nSome penalties may not work with some solvers. See the parameter\n`solver` below, to know the compatibility between the penalty and\nsolver.\n\n.. versionadded:: 0.19\nl1 penalty with SAGA solver (allowing 'multinomial' + L1)\n\n.. deprecated:: 1.8\n`penalty` was deprecated in version 1.8 and will be removed in 1.10.\nUse `l1_ratio` instead. `l1_ratio=0` for `penalty='l2'`, `l1_ratio=1` for\n`penalty='l1'` and `l1_ratio` set to any float between 0 and 1 for\n`'penalty='elasticnet'`.\n'deprecated'\n\n\n\nC C: float, default=1.0\n\nInverse of regularization strength; must be a positive float.\nLike in support vector machines, smaller values specify stronger\nregularization. `C=np.inf` results in unpenalized logistic regression.\nFor a visual example on the effect of tuning the `C` parameter\nwith an L1 penalty, see:\n:ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`.\n1.0\n\n\n\nl1_ratio l1_ratio: float, default=0.0\n\nThe Elastic-Net mixing parameter, with `0 &lt;= l1_ratio &lt;= 1`. Setting\n`l1_ratio=1` gives a pure L1-penalty, setting `l1_ratio=0` a pure L2-penalty.\nAny value between 0 and 1 gives an Elastic-Net penalty of the form\n`l1_ratio * L1 + (1 - l1_ratio) * L2`.\n\n.. warning::\nCertain values of `l1_ratio`, i.e. some penalties, may not work with some\nsolvers. See the parameter `solver` below, to know the compatibility between\nthe penalty and solver.\n\n.. versionchanged:: 1.8\nDefault value changed from None to 0.0.\n\n.. deprecated:: 1.8\n`None` is deprecated and will be removed in version 1.10. Always use\n`l1_ratio` to specify the penalty type.\n0.0\n\n\n\ndual dual: bool, default=False\n\nDual (constrained) or primal (regularized, see also\n:ref:`this equation `) formulation. Dual formulation\nis only implemented for l2 penalty with liblinear solver. Prefer `dual=False`\nwhen n_samples &gt; n_features.\nFalse\n\n\n\ntol tol: float, default=1e-4\n\nTolerance for stopping criteria.\n0.0001\n\n\n\nfit_intercept fit_intercept: bool, default=True\n\nSpecifies if a constant (a.k.a. bias or intercept) should be\nadded to the decision function.\nTrue\n\n\n\nintercept_scaling intercept_scaling: float, default=1\n\nUseful only when the solver `liblinear` is used\nand `self.fit_intercept` is set to `True`. In this case, `x` becomes\n`[x, self.intercept_scaling]`,\ni.e. a \"synthetic\" feature with constant value equal to\n`intercept_scaling` is appended to the instance vector.\nThe intercept becomes\n``intercept_scaling * synthetic_feature_weight``.\n\n.. note::\nThe synthetic feature weight is subject to L1 or L2\nregularization as all other features.\nTo lessen the effect of regularization on synthetic feature weight\n(and therefore on the intercept) `intercept_scaling` has to be increased.\n1\n\n\n\nclass_weight class_weight: dict or 'balanced', default=None\n\nWeights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one.\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\n\n.. versionadded:: 0.17\n*class_weight='balanced'*\nNone\n\n\n\nrandom_state random_state: int, RandomState instance, default=None\n\nUsed when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\ndata. See :term:`Glossary ` for details.\nNone\n\n\n\nsolver solver: {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'}, default='lbfgs'\n\nAlgorithm to use in the optimization problem. Default is 'lbfgs'.\nTo choose a solver, you might want to consider the following aspects:\n\n- 'lbfgs' is a good default solver because it works reasonably well for a wide\nclass of problems.\n- For :term:`multiclass` problems (`n_classes &gt;= 3`), all solvers except\n'liblinear' minimize the full multinomial loss, 'liblinear' will raise an\nerror.\n- 'newton-cholesky' is a good choice for\n`n_samples` &gt;&gt; `n_features * n_classes`, especially with one-hot encoded\ncategorical features with rare categories. Be aware that the memory usage\nof this solver has a quadratic dependency on `n_features * n_classes`\nbecause it explicitly computes the full Hessian matrix.\n- For small datasets, 'liblinear' is a good choice, whereas 'sag'\nand 'saga' are faster for large ones;\n- 'liblinear' can only handle binary classification by default. To apply a\none-versus-rest scheme for the multiclass setting one can wrap it with the\n:class:`~sklearn.multiclass.OneVsRestClassifier`.\n\n.. warning::\nThe choice of the algorithm depends on the penalty chosen (`l1_ratio=0`\nfor L2-penalty, `l1_ratio=1` for L1-penalty and `0 &lt; l1_ratio &lt; 1` for\nElastic-Net) and on (multinomial) multiclass support:\n\n================= ======================== ======================\nsolver l1_ratio multinomial multiclass\n================= ======================== ======================\n'lbfgs' l1_ratio=0 yes\n'liblinear' l1_ratio=1 or l1_ratio=0 no\n'newton-cg' l1_ratio=0 yes\n'newton-cholesky' l1_ratio=0 yes\n'sag' l1_ratio=0 yes\n'saga' 0&lt;=l1_ratio&lt;=1 yes\n================= ======================== ======================\n\n.. note::\n'sag' and 'saga' fast convergence is only guaranteed on features\nwith approximately the same scale. You can preprocess the data with\na scaler from :mod:`sklearn.preprocessing`.\n\n.. seealso::\nRefer to the :ref:`User Guide ` for more\ninformation regarding :class:`LogisticRegression` and more specifically the\n:ref:`Table `\nsummarizing solver/penalty supports.\n\n.. versionadded:: 0.17\nStochastic Average Gradient (SAG) descent solver. Multinomial support in\nversion 0.18.\n.. versionadded:: 0.19\nSAGA solver.\n.. versionchanged:: 0.22\nThe default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n.. versionadded:: 1.2\nnewton-cholesky solver. Multinomial support in version 1.6.\n'lbfgs'\n\n\n\nmax_iter max_iter: int, default=100\n\nMaximum number of iterations taken for the solvers to converge.\n5000\n\n\n\nverbose verbose: int, default=0\n\nFor the liblinear and lbfgs solvers set verbose to any positive\nnumber for verbosity.\n0\n\n\n\nwarm_start warm_start: bool, default=False\n\nWhen set to True, reuse the solution of the previous call to fit as\ninitialization, otherwise, just erase the previous solution.\nUseless for liblinear solver. See :term:`the Glossary `.\n\n.. versionadded:: 0.17\n*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\nFalse\n\n\n\nn_jobs n_jobs: int, default=None\n\nDoes not have any effect.\n\n.. deprecated:: 1.8\n`n_jobs` is deprecated in version 1.8 and will be removed in 1.10.\nNone"
  },
  {
    "objectID": "slides-01.html#unseen-messages",
    "href": "slides-01.html#unseen-messages",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Unseen messages",
    "text": "Unseen messages\n\nNow use the trained model to predict targets of unseen messages:\n\n\n\n\n\n\n\n\n\n\nsms\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one m...\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok."
  },
  {
    "objectID": "slides-01.html#predicting-on-unseen-data",
    "href": "slides-01.html#predicting-on-unseen-data",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting on unseen data",
    "text": "Predicting on unseen data\nThe model is accurately predicting labels for the unseen text messages above!\n\n\n\n\n\n\n\n¬†\nsms\nspam_predictions\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\nham\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\nham\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\nspam\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\nham"
  },
  {
    "objectID": "slides-01.html#examplel-text-classification-with-llms",
    "href": "slides-01.html#examplel-text-classification-with-llms",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Examplel: Text classification with LLMs",
    "text": "Examplel: Text classification with LLMs\n¬†\n\nLLMs = Large Language Models\n\n\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n# Sentiment analysis pipeline\nanalyzer = pipeline(\"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english')\nanalyzer([\"I asked my model to predict my future, and it said '404: Life not found.'\",\n          '''Machine learning is just like cooking‚Äîsometimes you follow the recipe, \n            and other times you just hope for the best!.'''])\n\n[{'label': 'NEGATIVE', 'score': 0.995707631111145},\n {'label': 'POSITIVE', 'score': 0.9994770884513855}]"
  },
  {
    "objectID": "slides-01.html#zero-shot-learning",
    "href": "slides-01.html#zero-shot-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Zero-shot learning",
    "text": "Zero-shot learning\n\n\n\nNow suppose you want to identify the emotion expressed in the text rather than just positive or negative.\n\n\n\n\n['im feeling rather rotten so im not very ambitious right now',\n 'im updating my blog because i feel shitty',\n 'i never make her separate from me because i don t ever want her to feel like i m ashamed with her',\n 'i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',\n 'i was feeling a little vain when i did this one',\n 'i cant walk into a shop anywhere where i do not feel uncomfortable',\n 'i felt anger when at the end of a telephone call',\n 'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',\n 'i like to have the same breathless feeling as a reader eager to see what will happen next',\n 'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer']"
  },
  {
    "objectID": "slides-01.html#zero-shot-learning-for-emotion-detection",
    "href": "slides-01.html#zero-shot-learning-for-emotion-detection",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Zero-shot learning for emotion detection",
    "text": "Zero-shot learning for emotion detection\n\n\n\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline \nimport torch\n\n#Load the pretrained model\nmodel_name = \"facebook/bart-large-mnli\"\nclassifier = pipeline('zero-shot-classification', model=model_name)\nexs = dataset[\"test\"][\"text\"][10:20]\ncandidate_labels = [\"sadness\", \"joy\", \"love\",\"anger\", \"fear\", \"surprise\"]\noutputs = classifier(exs, candidate_labels)"
  },
  {
    "objectID": "slides-01.html#zero-shot-learning-for-emotion-detection-1",
    "href": "slides-01.html#zero-shot-learning-for-emotion-detection-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Zero-shot learning for emotion detection",
    "text": "Zero-shot learning for emotion detection\n\n\n\n\n\n\n\n\n\n\n\n\nsequence\nlabels\nscores\n\n\n\n\n0\ni don t feel particularly agitated\n[surprise, anger, joy, sadness, fear, love]\n[0.3600873053073883, 0.3019026815891266, 0.11901309341192245, 0.11381532996892929, 0.060391392558813095, 0.04479021206498146]\n\n\n1\ni feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey\n[joy, love, surprise, fear, sadness, anger]\n[0.36994317173957825, 0.28871580958366394, 0.25607895851135254, 0.042923394590616226, 0.03344894200563431, 0.008889704011380672]\n\n\n2\ni pay attention it deepens into a feeling of being invaded and helpless\n[fear, surprise, sadness, anger, joy, love]\n[0.3414689302444458, 0.30880674719810486, 0.2561693787574768, 0.07989845424890518, 0.00784482154995203, 0.005811676848679781]\n\n\n3\ni just feel extremely comfortable with the group of people that i dont even need to hide myself\n[joy, surprise, love, sadness, anger, fear]\n[0.33052244782447815, 0.29472336173057556, 0.15343077480793, 0.07691455632448196, 0.07596749812364578, 0.06844136863946915]\n\n\n4\ni find myself in the odd position of feeling supportive of\n[surprise, joy, fear, love, sadness, anger]\n[0.8287991881370544, 0.043179433792829514, 0.039773616939783096, 0.03141303360462189, 0.031412284821271896, 0.025422412902116776]\n\n\n5\ni was feeling as heartbroken as im sure katniss was\n[sadness, surprise, fear, love, anger, joy]\n[0.7667970657348633, 0.18184703588485718, 0.025871269404888153, 0.011756835505366325, 0.00817161239683628, 0.00555615546181798]\n\n\n6\ni feel a little mellow today\n[surprise, joy, love, fear, sadness, anger]\n[0.4937363266944885, 0.2632198929786682, 0.11367864906787872, 0.06402146071195602, 0.050954896956682205, 0.014388857409358025]\n\n\n7\ni feel like my only role now would be to tear your sails with my pessimism and discontent\n[sadness, anger, surprise, fear, joy, love]\n[0.6992800235748291, 0.20048724114894867, 0.06185886263847351, 0.03287408500909805, 0.0036468561738729477, 0.0018528576474636793]\n\n\n8\ni feel just bcoz a fight we get mad to each other n u wanna make a publicity n let the world knows about our fight\n[anger, surprise, sadness, fear, joy, love]\n[0.6029898524284363, 0.19827152788639069, 0.10198832303285599, 0.08116993308067322, 0.010117105208337307, 0.005463324021548033]\n\n\n9\ni feel like reds and purples are just so rich and kind of perfect\n[joy, surprise, love, anger, fear, sadness]\n[0.3644145131111145, 0.3051210939884186, 0.19462482631206512, 0.055566418915987015, 0.05413556843996048, 0.02613767609000206]"
  },
  {
    "objectID": "slides-01.html#example-predicting-labels-of-a-given-image",
    "href": "slides-01.html#example-predicting-labels-of-a-given-image",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Example: Predicting labels of a given image",
    "text": "Example: Predicting labels of a given image\n\nSuppose you have a bunch of animal images. You do not have any labels associated with them and you want to predict labels of these images.\nWe can use machine learning to predict labels of these images using a technique called transfer learning.\n\n\n\n\n\n\n\n\n\n\n\n                         Class  Probability score\n                     tiger cat              0.636\n              tabby, tabby cat              0.174\nPembroke, Pembroke Welsh corgi              0.081\n               lynx, catamount              0.011\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                     Class  Probability score\n         cheetah, chetah, Acinonyx jubatus              0.994\n                  leopard, Panthera pardus              0.005\njaguar, panther, Panthera onca, Felis onca              0.001\n       snow leopard, ounce, Panthera uncia              0.000\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                                   Class  Probability score\n                                 macaque              0.885\npatas, hussar monkey, Erythrocebus patas              0.062\n      proboscis monkey, Nasalis larvatus              0.015\n                       titi, titi monkey              0.010\n--------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n                        Class  Probability score\nWalker hound, Walker foxhound              0.582\n             English foxhound              0.144\n                       beagle              0.068\n                  EntleBucher              0.059\n--------------------------------------------------------------\n\n\n\n:::"
  },
  {
    "objectID": "slides-01.html#finding-groups-in-food-images",
    "href": "slides-01.html#finding-groups-in-food-images",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Finding groups in food images",
    "text": "Finding groups in food images"
  },
  {
    "objectID": "slides-01.html#k-means-on-food-dataset",
    "href": "slides-01.html#k-means-on-food-dataset",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "K-Means on food dataset",
    "text": "K-Means on food dataset\n\n\n\ndensenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\ndensenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer\n\n\nZ_food = get_features_unsup(densenet, food_inputs)\nk = 5\nkm = KMeans(n_clusters=k, n_init='auto', random_state=123)\nkm.fit(Z_food)\n\nKMeans(n_clusters=5, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans?Documentation for KMeansiFitted\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nn_clusters n_clusters: int, default=8\n\nThe number of clusters to form as well as the number of\ncentroids to generate.\n\nFor an example of how to choose an optimal value for `n_clusters` refer to\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.\n5\n\n\n\ninit init: {'k-means++', 'random'}, callable or array-like of shape (n_clusters, n_features), default='k-means++'\n\nMethod for initialization:\n\n* 'k-means++' : selects initial cluster centroids using sampling based on an empirical probability distribution of the points' contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is \"greedy k-means++\". It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.\n\n* 'random': choose `n_clusters` observations (rows) at random from data for the initial centroids.\n\n* If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n\n* If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization.\n\nFor an example of how to use the different `init` strategies, see\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.\n\nFor an evaluation of the impact of initialization, see the example\n:ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.\n'k-means++'\n\n\n\nn_init n_init: 'auto' or int, default='auto'\n\nNumber of times the k-means algorithm is run with different centroid\nseeds. The final results is the best output of `n_init` consecutive runs\nin terms of inertia. Several runs are recommended for sparse\nhigh-dimensional problems (see :ref:`kmeans_sparse_high_dim`).\n\nWhen `n_init='auto'`, the number of runs depends on the value of init:\n10 if using `init='random'` or `init` is a callable;\n1 if using `init='k-means++'` or `init` is an array-like.\n\n.. versionadded:: 1.2\nAdded 'auto' option for `n_init`.\n\n.. versionchanged:: 1.4\nDefault value for `n_init` changed to `'auto'`.\n'auto'\n\n\n\nmax_iter max_iter: int, default=300\n\nMaximum number of iterations of the k-means algorithm for a\nsingle run.\n300\n\n\n\ntol tol: float, default=1e-4\n\nRelative tolerance with regards to Frobenius norm of the difference\nin the cluster centers of two consecutive iterations to declare\nconvergence.\n0.0001\n\n\n\nverbose verbose: int, default=0\n\nVerbosity mode.\n0\n\n\n\nrandom_state random_state: int, RandomState instance or None, default=None\n\nDetermines random number generation for centroid initialization. Use\nan int to make the randomness deterministic.\nSee :term:`Glossary `.\n123\n\n\n\ncopy_x copy_x: bool, default=True\n\nWhen pre-computing distances it is more numerically accurate to center\nthe data first. If copy_x is True (default), then the original data is\nnot modified. If False, the original data is modified, and put back\nbefore the function returns, but small numerical differences may be\nintroduced by subtracting and then adding the data mean. Note that if\nthe original data is not C-contiguous, a copy will be made even if\ncopy_x is False. If the original data is sparse, but not in CSR format,\na copy will be made even if copy_x is False.\nTrue\n\n\n\nalgorithm algorithm: {\"lloyd\", \"elkan\"}, default=\"lloyd\"\n\nK-means algorithm to use. The classical EM-style algorithm is `\"lloyd\"`.\nThe `\"elkan\"` variation can be more efficient on some datasets with\nwell-defined clusters, by using the triangle inequality. However it's\nmore memory intensive due to the allocation of an extra array of shape\n`(n_samples, n_clusters)`.\n\n.. versionchanged:: 0.18\nAdded Elkan algorithm\n\n.. versionchanged:: 1.1\nRenamed \"full\" to \"lloyd\", and deprecated \"auto\" and \"full\".\nChanged \"auto\" to use \"lloyd\" instead of \"elkan\".\n'lloyd'"
  },
  {
    "objectID": "slides-01.html#examining-food-clusters",
    "href": "slides-01.html#examining-food-clusters",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Examining food clusters",
    "text": "Examining food clusters\n\n\n\n\nfor cluster in range(k):\n    get_cluster_images(km, Z_food, X_food, cluster, n_img=6)\n\n39\nImage indices:  [ 39 197  12  14 138 181]\n\n\n\n\n\n\n\n\n\n228\nImage indices:  [228  65 128  54 175 260]\n\n\n\n\n\n\n\n\n\n138\nImage indices:  [138  54 185 278  39  89]\n\n\n\n\n\n\n\n\n\n193\nImage indices:  [193  39 145 212 169 108]\n\n\n\n\n\n\n\n\n\n120\nImage indices:  [120 268 244  94  72  87]"
  },
  {
    "objectID": "slides-01.html#questions-for-you",
    "href": "slides-01.html#questions-for-you",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "‚ùì‚ùì Questions for you",
    "text": "‚ùì‚ùì Questions for you\nSelect all that apply: Which problems are suitable for ML?\n\n\nChecking if a UBC email address ends with @student.ubc.ca before allowing login\n\n\nDeciding which students should be awarded a scholarship based on their personal essays\n\n\nPredicting which songs you‚Äôll like based on your Spotify listening history\n\n\nDetecting plagiarism by checking if two essays are exactly identical\n\n\nAutomatically tagging photos of your friends on Instagram"
  },
  {
    "objectID": "slides-01.html#summary-when-is-ml-suitable",
    "href": "slides-01.html#summary-when-is-ml-suitable",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Summary: When is ML suitable?",
    "text": "Summary: When is ML suitable?\n\n\n\n\n\n\n\nApproach\nBest Used When‚Ä¶\n\n\n\n\nMachine Learning\nThe dataset is large and complex, and the decision rules are unknown, fuzzy, or too complex to define explicitly\n\n\nRule-based System\nThe logic is clear and deterministic, and the rules or thresholds are known and stable\n\n\nHuman Expert\nThe problem involves ethics, creativity, emotion, or ambiguity that can‚Äôt be formalized easily"
  },
  {
    "objectID": "slides-01.html#activity-2",
    "href": "slides-01.html#activity-2",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity 2",
    "text": "Activity 2\nThink of a problem you have come across in the past which could be solved using machine learning.\n\nWhat would be the input and output?\nHow do humans solve this now? Are there heuristics or rules?\nWhat kind of data do you have or could you collect?"
  },
  {
    "objectID": "slides-01.html#types-of-machine-learning",
    "href": "slides-01.html#types-of-machine-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Types of machine learning",
    "text": "Types of machine learning\nHere are some typical learning problems.\n\nSupervised learning (Gmail spam filtering)\nUnsupervised learning (Google News)\nReinforcement learning (AlphaGo)\nGenerative AI (ChatGPT)\nRecommendation systems (Amazon item recommendation system)"
  },
  {
    "objectID": "slides-01.html#what-is-supervised-learning",
    "href": "slides-01.html#what-is-supervised-learning",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is supervised learning?",
    "text": "What is supervised learning?\n\nTraining data comprises a set of observations (\\(X\\)) and their corresponding targets (\\(y\\)).\nWe wish to find a model function \\(f\\) that relates \\(X\\) to \\(y\\).\nWe use the model function to predict targets of new examples."
  },
  {
    "objectID": "slides-01.html#evas-questions",
    "href": "slides-01.html#evas-questions",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ü§î Eva‚Äôs questions",
    "text": "ü§î Eva‚Äôs questions\n\n\nAt this point, Eva is wondering about many questions.\n\nHow are we exactly ‚Äúlearning‚Äù whether a message is spam and ham?\nAre we expected to get correct predictions for all possible messages? How does it predict the label for a message it has not seen before?\n\nWhat if the model mis-labels an unseen example? For instance, what if the model incorrectly predicts a non-spam as a spam? What would be the consequences?\nHow do we measure the success or failure of spam identification?\nIf you want to use this model in the wild, how do you know how reliable it is?\n\nWould it be useful to know how confident the model is about the predictions rather than just a yes or a no?\n\nIt‚Äôs great to think about these questions right now. But Eva has to be patient. By the end of this course you‚Äôll know answers to many of these questions!"
  },
  {
    "objectID": "slides-01.html#what-is-machine-learning-ml",
    "href": "slides-01.html#what-is-machine-learning-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is Machine Learning (ML)?",
    "text": "What is Machine Learning (ML)?"
  },
  {
    "objectID": "slides-01.html#spam-prediction",
    "href": "slides-01.html#spam-prediction",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Spam prediction",
    "text": "Spam prediction\n\nSuppose you are given some data with labelled spam and non-spam messages\n\n\nCodeOutput\n\n\n\nsms_df = pd.read_csv(DATA_DIR + \"spam.csv\", encoding=\"latin-1\")\nsms_df = sms_df.drop(columns = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"])\nsms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\ntrain_df, test_df = train_test_split(sms_df, test_size=0.10, random_state=42)\n\n\n\n\n\n\n\n\n\ntarget\nsms\n\n\n\n\nspam\nLookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\n\n\nham\nAight, I'll hit you up when I get some cash\n\n\nham\nDon no da:)whats you plan?\n\n\nham\nGoing to take your babe out ?\n\n\nham\nNo need lar. Jus testing e phone card. Dunno network not gd i thk. Me waiting 4 my sis 2 finish bathing so i can bathe. Dun disturb u liao u cleaning ur room."
  },
  {
    "objectID": "slides-01.html#traditional-programming-vs.-ml",
    "href": "slides-01.html#traditional-programming-vs.-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Traditional programming vs.¬†ML",
    "text": "Traditional programming vs.¬†ML\n\nImagine writing a Python program for spam identification, i.e., whether a text message or an email is spam or non-spam.\nTraditional programming\n\nCome up with rules using human understanding of spam messages.\nTime consuming and hard to come up with robust set of rules.\n\nMachine learning\n\nCollect large amount of data of spam and non-spam emails and let the machine learning algorithm figure out rules."
  },
  {
    "objectID": "slides-01.html#lets-train-a-model-1",
    "href": "slides-01.html#lets-train-a-model-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Let‚Äôs train a model",
    "text": "Let‚Äôs train a model\n\nThere are several packages that help us perform machine learning.\n\n\nX_train, y_train = train_df[\"sms\"], train_df[\"target\"]\nX_test, y_test = test_df[\"sms\"], test_df[\"target\"]\nclf = make_pipeline(CountVectorizer(max_features=5000), LogisticRegression(max_iter=5000))\nclf.fit(X_train, y_train); # Training the model"
  },
  {
    "objectID": "slides-01.html#unseen-messages-1",
    "href": "slides-01.html#unseen-messages-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Unseen messages",
    "text": "Unseen messages\n\nNow use the trained model to predict targets of unseen messages:\n\n\n\n\n\n\n\n\n\n\nsms\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one m...\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok."
  },
  {
    "objectID": "slides-01.html#predicting-on-unseen-data-1",
    "href": "slides-01.html#predicting-on-unseen-data-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Predicting on unseen data",
    "text": "Predicting on unseen data\nThe model is accurately predicting labels for the unseen text messages above!\n\n\n\n\n\n\n\n¬†\nsms\nspam_predictions\n\n\n\n\n3245\nFunny fact Nobody teaches volcanoes 2 erupt, tsunamis 2 arise, hurricanes 2 sway aroundn no 1 teaches hw 2 choose a wife Natural disasters just happens\nham\n\n\n944\nI sent my scores to sophas and i had to do secondary application for a few schools. I think if you are thinking of applying, do a research on cost also. Contact joke ogunrinde, her school is one me the less expensive ones\nham\n\n\n1044\nWe know someone who you know that fancies you. Call 09058097218 to find out who. POBox 6, LS15HB 150p\nspam\n\n\n2484\nOnly if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\nham"
  },
  {
    "objectID": "slides-01.html#a-different-way-to-solve-problems",
    "href": "slides-01.html#a-different-way-to-solve-problems",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "A different way to solve problems",
    "text": "A different way to solve problems\nMachine learning uses computer programs to model data. It can be used to extract hidden patterns, make predictions in new situation, or generate novel content.\n\nA field of study that gives computers the ability to learn without being explicitly programmed.  ‚Äì Arthur Samuel (1959)"
  },
  {
    "objectID": "slides-01.html#ml-vs.-traditional-programming",
    "href": "slides-01.html#ml-vs.-traditional-programming",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ML vs.¬†traditional programming",
    "text": "ML vs.¬†traditional programming\n\nWith machine learning, you‚Äôre likely to\n\nSave time\nCustomize and scale products"
  },
  {
    "objectID": "slides-01.html#prevalence-of-ml",
    "href": "slides-01.html#prevalence-of-ml",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Prevalence of ML",
    "text": "Prevalence of ML\nLet‚Äôs look at some examples."
  },
  {
    "objectID": "slides-01.html#activity-for-what-type-of-problems-ml-is-appropriate-5-mins",
    "href": "slides-01.html#activity-for-what-type-of-problems-ml-is-appropriate-5-mins",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Activity: For what type of problems ML is appropriate? (~5 mins)",
    "text": "Activity: For what type of problems ML is appropriate? (~5 mins)\nDiscuss with your neighbour for which of the following problems you would use machine learning\n\nFinding a list of prime numbers up to a limit\nGiven an image, automatically identifying and labeling objects in the image\nFinding the distance between two nodes in a graph"
  },
  {
    "objectID": "slides-01.html#types-of-machine-learning-1",
    "href": "slides-01.html#types-of-machine-learning-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Types of machine learning",
    "text": "Types of machine learning\nHere are some typical learning problems.\n\nSupervised learning (Gmail spam filtering)\n\nTraining a model from input data and its corresponding targets to predict targets for new examples.\n\n\nUnsupervised learning (Google News)\n\nTraining a model to find patterns in a dataset, typically an unlabeled dataset.\n\nReinforcement learning (AlphaGo)\n\nA family of algorithms for finding suitable actions to take in a given situation in order to maximize a reward.\n\nRecommendation systems (Amazon item recommendation system)\n\nPredict the ‚Äúrating‚Äù or ‚Äúpreference‚Äù a user would give to an item."
  },
  {
    "objectID": "slides-01.html#what-is-supervised-learning-1",
    "href": "slides-01.html#what-is-supervised-learning-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "What is supervised learning?",
    "text": "What is supervised learning?\n\nTraining data comprises a set of observations (\\(X\\)) and their corresponding targets (\\(y\\)).\nWe wish to find a model function \\(f\\) that relates \\(X\\) to \\(y\\).\nWe use the model function to predict targets of new examples."
  },
  {
    "objectID": "slides-01.html#evas-questions-1",
    "href": "slides-01.html#evas-questions-1",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "ü§î Eva‚Äôs questions",
    "text": "ü§î Eva‚Äôs questions\n\n\nAt this point, Eva is wondering about many questions.\n\nHow are we exactly ‚Äúlearning‚Äù whether a message is spam and ham?\nAre we expected to get correct predictions for all possible messages? How does it predict the label for a message it has not seen before?\n\nWhat if the model mis-labels an unseen example? For instance, what if the model incorrectly predicts a non-spam as a spam? What would be the consequences?\nHow do we measure the success or failure of spam identification?\nIf you want to use this model in the wild, how do you know how reliable it is?\n\nWould it be useful to know how confident the model is about the predictions rather than just a yes or a no?\n\nIt‚Äôs great to think about these questions right now. But Eva has to be patient. By the end of this course you‚Äôll know answers to many of these questions!"
  },
  {
    "objectID": "slides-01.html#looking-ahead-to-next-class",
    "href": "slides-01.html#looking-ahead-to-next-class",
    "title": "Lecture 1: Introduction to CPSC 330",
    "section": "Looking ahead to next class",
    "text": "Looking ahead to next class\nIt is very important that you watch the assigned pre-lecture videos before class!"
  },
  {
    "objectID": "slides-04.html#finish-up-lecture-3-demo",
    "href": "slides-04.html#finish-up-lecture-3-demo",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Finish up Lecture 3 Demo",
    "text": "Finish up Lecture 3 Demo\nFor this demo, each student should click this link to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today."
  },
  {
    "objectID": "slides-04.html#recap-clicker-4.0a",
    "href": "slides-04.html#recap-clicker-4.0a",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap: Clicker 4.0a",
    "text": "Recap: Clicker 4.0a\nParticipate using Agora (code: agentic)\nWhich of the following scenarios do NOT necessarily imply overfitting?\n\n\nTraining accuracy is very high (0.98) while validation accuracy is much lower (0.60).\n\n\nIn a wildlife classifier, the model predicts ‚Äúwolf‚Äù whenever there‚Äùs snow in the background, because all wolf photos were taken in snowy regions.\n\n\nThe decision boundary of a classifier is wiggly and highly irregular.\n\n\nTraining and validation accuracies are both approximately 0.88.\n\n\nA cancer detection model learns that ‚Äúa ruler in the corner of the X-ray‚Äù means positive, because doctors tended to measure suspicious cases."
  },
  {
    "objectID": "slides-04.html#recap-clicker-4.0b",
    "href": "slides-04.html#recap-clicker-4.0b",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap: Clicker 4.0b",
    "text": "Recap: Clicker 4.0b\nParticipate using Agora (code: agentic)\nWhich of the following statements about overfitting is true?\n\n\nOverfitting makes the model more accurate on both training and unseen data.\n\n\nOverfitting means the model captures noise or irrelevant details from the training data.\n\n\nOverfitting is desirable because it reduces both training and test error.\n\n\nIn real-world problems, models are always at risk of overfitting if not properly validated."
  },
  {
    "objectID": "slides-04.html#recap-clicker-4.0c",
    "href": "slides-04.html#recap-clicker-4.0c",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Recap: Clicker 4.0c",
    "text": "Recap: Clicker 4.0c\nParticipate using Agora (code: agentic)\nHow might one address the issue of underfitting in a machine learning model.\n\n\nIntroduce more noise to the training data.\n\n\nRemove features that might be relevant to the prediction.\n\n\nIncrease the model‚Äôs complexity (e.g., more parameters, features, or deeper trees)\n\n\nUse a smaller dataset for training."
  },
  {
    "objectID": "slides-04.html#the-fundamental-tradeoff",
    "href": "slides-04.html#the-fundamental-tradeoff",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "The fundamental tradeoff",
    "text": "The fundamental tradeoff\n\n\n\n\n\n\n\n\n\nAs you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.\n\nHow to pick a model?"
  },
  {
    "objectID": "slides-04.html#overfitting-and-underfitting",
    "href": "slides-04.html#overfitting-and-underfitting",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nAn overfit model matches the training set so closely that it fails to make correct predictions on new unseen data.\n\nAn underfit model is too simple and does not even make good predictions on the training data"
  },
  {
    "objectID": "slides-04.html#overfitting-and-underfitting-1",
    "href": "slides-04.html#overfitting-and-underfitting-1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Overfitting and underfitting",
    "text": "Overfitting and underfitting\n\nSource"
  },
  {
    "objectID": "slides-04.html#clicker-4.1",
    "href": "slides-04.html#clicker-4.1",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Clicker 4.1",
    "text": "Clicker 4.1\nParticipate using Agora (code: agentic)\nSelect all of the following statements which are TRUE.\n\n\nAnalogy-based models find examples from the test set that are most similar to the query example we are predicting.\n\n\nEuclidean distance will always have a non-negative value.\n\n\nWith \\(k\\)-NN, setting the hyperparameter \\(k\\) to larger values typically reduces training error.\n\n\nSimilar to decision trees, \\(k\\)-NNs finds a small set of good features.\n\n\nIn \\(k\\)-NN, with \\(k &gt; 1\\), the classification of the closest neighbour to the test example always contributes the most to the prediction."
  },
  {
    "objectID": "slides-04.html#clicker-4.2",
    "href": "slides-04.html#clicker-4.2",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Clicker 4.2",
    "text": "Clicker 4.2\nParticipate using Agora (code: agentic)\nSelect all of the following statements which are TRUE.\n\n\n\\(k\\)-NN may perform poorly in high-dimensional space (say, d &gt; 1000).\n\n\nIn sklearn‚Äôs SVC classifier, large values of gamma tend to result in higher training score but probably lower validation score.\n\n\nIf we increase both gamma and C, we can‚Äôt be certain if the model becomes more complex or less complex."
  },
  {
    "objectID": "slides-04.html#similarity-based-algorithms",
    "href": "slides-04.html#similarity-based-algorithms",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Similarity-based algorithms",
    "text": "Similarity-based algorithms\n\nUse similarity or distance metrics to predict targets.\nExamples: \\(k\\)-nearest neighbors, Support Vector Machines (SVMs) with RBF Kernel."
  },
  {
    "objectID": "slides-04.html#k-nearest-neighbours",
    "href": "slides-04.html#k-nearest-neighbours",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "\\(k\\)-nearest neighbours",
    "text": "\\(k\\)-nearest neighbours\n\nClassifies an object based on the majority label among its \\(k\\) closest neighbors.\nMain hyperparameter: \\(k\\) or n_neighbors in sklearn\nDistance Metrics: Euclidean\nStrengths: simple and intuitive, can learn complex decision boundaries\nChallenges: Sensitive to the choice of distance metric and scaling (coming up)."
  },
  {
    "objectID": "slides-04.html#curse-of-dimensionality",
    "href": "slides-04.html#curse-of-dimensionality",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\n\nAs dimensionality increases, the volume of the space increases exponentially, making the data sparse.\nDistance metrics lose meaning\n\nAccidental similarity swamps out meaningful similarity\nAll points become almost equidistant.\n\nOverfitting becomes likely: Harder to generalize with high-dimensional data.\nHow to deal with this?\n\nDimensionality reduction (PCA) (not covered in this course)\nFeature selection techniques."
  },
  {
    "objectID": "slides-04.html#svms-with-rbf-kernel",
    "href": "slides-04.html#svms-with-rbf-kernel",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "SVMs with RBF kernel",
    "text": "SVMs with RBF kernel\n\nRBF Kernel: Radial Basis Function, a way to transform data into higher dimensions implicitly.\nStrengths\n\nEffective in high-dimensional and sparse data\nGood performance on non-linear problems.\n\nHyperparameters:\n\n\\(C\\): Regularization parameter (trade-off between correct classification of training examples and maximization of the decision margin).\ngamma (\\(\\gamma\\)): controls how fast the similarity decays with distance"
  },
  {
    "objectID": "slides-04.html#intuition-of-c-and-gamma-in-svm-rbf",
    "href": "slides-04.html#intuition-of-c-and-gamma-in-svm-rbf",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Intuition of C and gamma in SVM RBF",
    "text": "Intuition of C and gamma in SVM RBF\n\nC (Regularization): Controls the trade-off between perfect training accuracy and having a simpler decision boundary.\n\nHigh C: Strict, complex boundary (overfitting risk).\nLow C: More errors allowed, smoother boundary (generalizes better).\n\nGamma (Kernel Width): Controls the influence of individual data points.\n\nHigh Gamma: Points have local impact, complex boundary.\nLow Gamma: Points affect broader areas, smoother boundary.\n\nKey trade-off: Proper balance between C and gamma is crucial for avoiding overfitting or underfitting."
  },
  {
    "objectID": "slides-04.html#break",
    "href": "slides-04.html#break",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Break",
    "text": "Break\nLet‚Äôs take a break!"
  },
  {
    "objectID": "slides-04.html#group-work-class-demo-live-coding",
    "href": "slides-04.html#group-work-class-demo-live-coding",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Group Work: Class Demo & Live Coding",
    "text": "Group Work: Class Demo & Live Coding\nFor this demo, each student should click this link to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today."
  },
  {
    "objectID": "slides-04.html#supervised-models-we-have-seen",
    "href": "slides-04.html#supervised-models-we-have-seen",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Supervised models we have seen",
    "text": "Supervised models we have seen\n\nDecision trees: Split data into subsets based on feature values to create decision rules\n\\(k\\)-NNs: Classify based on the majority vote from \\(k\\) nearest neighbors\nSVM RBFs: Create a boundary using an RBF kernel to separate classes"
  },
  {
    "objectID": "slides-04.html#comparison-of-models-activity",
    "href": "slides-04.html#comparison-of-models-activity",
    "title": "Lecture 4: \\(k\\)-nearest neighbours and SVM RBFs",
    "section": "Comparison of models (activity)",
    "text": "Comparison of models (activity)\n\n\n\n\n\n\n\n\n\nModel\nParameters and hyperparameters\nStrengths\nWeaknesses\n\n\n\n\nDecision Trees\n\n\n\n\n\nKNNs\n\n\n\n\n\nSVM RBF"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Overview",
    "section": "",
    "text": "Title\n\n\n\nDescription\n\n\n\n\n\n\n\n\nLecture 1: Introduction to CPSC 330\n\n\nWhat is machine learning, types of machine learning, learning to navigate through the course materials, getting familiar with the course policies\n\n\n\n\n\n\nLecture 2: Terminology, Baselines, Decision Trees\n\n\nTerminology and Decision Trees\n\n\n\n\n\n\nLecture 3: ML fundamentals\n\n\ngeneralization, data splitting, overfitting, underfitting, the fundamental tradeoff, the golden rule\n\n\n\n\n\n\nLecture 4: \\(k\\)-nearest neighbours and SVM RBFs\n\n\nintroduction to KNNs, hyperparameter n_neighbours or \\(k\\), C and gamma hyperparameters of SVM RBF, decision boundaries with different values of hyperparameters.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to CPSC330! Here, you‚Äôll find slides for CPSC 330. These slides are based on the notes presented here.\n\nClass times üïò 3:30 PM to 5 PM, Tuesday and Thursday\nWhere? üìç Hugh Dempster Pavillion 310, 6245 Agronomy Road, Vancouver, BC V6T 1Z4"
  },
  {
    "objectID": "slides-02.html#announcements",
    "href": "slides-02.html#announcements",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Announcements",
    "text": "Announcements\n\nThings due this week\n\nHomework 1 (hw1): Due Jan 12 , 11:59 pm\nThere are some assert statements in the notebook you can use to check your work! If they fail, you didn‚Äôt get the exercise correct \n\nYou can find the tentative due dates for all deliverables here.\nPlease monitor Ed Discussion (especially pinned posts and instructor posts) for announcements.\nI‚Äôll assume that you‚Äôve watched the pre-lecture videos."
  },
  {
    "objectID": "slides-02.html#participation-marks-5-in-section-202",
    "href": "slides-02.html#participation-marks-5-in-section-202",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Participation marks (5%) in Section 202",
    "text": "Participation marks (5%) in Section 202\n\nIn this section (202) we will not be using iClickers for ‚Äúattendance‚Äù\nInstead, you will have weekly ‚ÄúLearning Logs‚Äù where you will reflect on the material covered for the week\nThe first Learning Log will be released tonight, and will be due Sunday Jan 11, 11:59 PM\nOnly students registered in section 202 will see these on PrairieLearn"
  },
  {
    "objectID": "slides-02.html#gradescope",
    "href": "slides-02.html#gradescope",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Gradescope",
    "text": "Gradescope\nMake sure you can submit your assignment before the hw1 due date!\n\nIt is required for you to work in a GitHub repository, please maintain your GitHub repo up-to-date.\nSome students are having trouble getting registered on Gradescope, we‚Äôre working on a fix - come see me in the break or after class!"
  },
  {
    "objectID": "slides-02.html#checklist-for-you-in-the-first-week",
    "href": "slides-02.html#checklist-for-you-in-the-first-week",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Checklist for you in the first week",
    "text": "Checklist for you in the first week\n\nAre you able to access course Canvas shell?\nAre you able to access course forum: Ed Discussion?\nAre you able to access Gradescope? (If not, refer to the Gradescope Student Guide.) \nDid you follow the setup instructions here to create a course conda environment on your computer?  \nAre you almost finished or at least started with homework 1?"
  },
  {
    "objectID": "slides-02.html#suggested-workflow-for-working-with-jupyter-notebooks",
    "href": "slides-02.html#suggested-workflow-for-working-with-jupyter-notebooks",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Suggested Workflow for working with Jupyter Notebooks",
    "text": "Suggested Workflow for working with Jupyter Notebooks\n\nCreate a folder on your computer that will have all the CPSC 330 repos:\n\n~/School/Year3/CPSC330/ &lt;‚Äì Consider this your CPSC parent folder\n\nCreate subfolders for: hw, class, practice\nIn the hw folder, you will then clone hw1, hw2, hw3, etc‚Ä¶\nIn the class folder, you will clone the cpsc330-2025W2 repo which contains all the class jupyter notebooks\n\nDo not make any changes to files in this directory/repo, you will have trouble when you pull stuff during each class.\nIf you did make changes, you can reset to the last commit and DESTROY any changes you made (be careful with this command) using: git reset --hard\n\nIn the practice folder, you can copy any notebooks (.ipynb) and files (like data/*.csv) you want to try running locally and experiment"
  },
  {
    "objectID": "slides-02.html#learning-outcomes",
    "href": "slides-02.html#learning-outcomes",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "üéØ Learning Outcomes",
    "text": "üéØ Learning Outcomes\nBy the end of this lesson, you will be able to:\n\nDefine key machine learning terminology:\nfeatures, targets, predictions, training, error, classification vs.¬†regression, supervised vs.¬†unsupervised learning, hyperparameters vs.¬†parameters, baselines, decision boundaries\nBuild a simple machine learning model in scikit-learn, explaining the fit‚Äìpredict workflow and evaluating performance with the score method\nDescribe at a high level how decision trees are trained (fitting) and how they make predictions\nImplement and visualize decision trees in scikit-learn using DecisionTreeClassifier and DecisionTreeRegressor"
  },
  {
    "objectID": "slides-02.html#recap-what-is-ml",
    "href": "slides-02.html#recap-what-is-ml",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: What is ML?",
    "text": "Recap: What is ML?\n\nML uses data to build models that find patterns, make predictions, or generate content.\nIt helps computers learn from data to make decisions.\nNo one model works for every situation."
  },
  {
    "objectID": "slides-02.html#class-participation-using-agora",
    "href": "slides-02.html#class-participation-using-agora",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Class Participation using Agora",
    "text": "Class Participation using Agora\n\nVisit: https://agora.students.cs.ubc.ca\nLogin with your UBC CWL\nUse enrol code: agentic"
  },
  {
    "objectID": "slides-02.html#clicker-2.1-ml-or-not",
    "href": "slides-02.html#clicker-2.1-ml-or-not",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Clicker 2.1: ML or not",
    "text": "Clicker 2.1: ML or not\nSelect all of the following statements which are suitable problems for machine learning.\n\n\nIdentifying objects within digital images, such as facial recognition in security systems or categorizing images based on content.\n\n\nDetermining if individuals meet the necessary criteria for government or financial services based on strict guidelines.\n\n\nIdentifying unusual patterns that may indicate fraudulent transactions in banking and finance.\n\n\nAutomatically analyzing images from MRIs, CT scans, or X-rays to detect abnormalities like tumors or fractures.\n\n\nAddressing mental health issues where human empathy, understanding, and adaptability are key."
  },
  {
    "objectID": "slides-02.html#therapists-using-chatgpt-secretly",
    "href": "slides-02.html#therapists-using-chatgpt-secretly",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Therapists using ChatGPT secretly üòî",
    "text": "Therapists using ChatGPT secretly üòî"
  },
  {
    "objectID": "slides-02.html#recap-when-is-ml-suitable",
    "href": "slides-02.html#recap-when-is-ml-suitable",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: When is ML suitable?",
    "text": "Recap: When is ML suitable?\n\nML excels when the problem involve identifying complex patterns or relationships in large datasets that are difficult for humans to discern manually.\nRule-based systems are suitable where clear and deterministic rules can be defined. Good for structured decision making.\nHuman experts are good with problems which require deep contextual understanding, ethical judgment, creative input, or emotional intelligence."
  },
  {
    "objectID": "slides-02.html#recap-supervised-learning",
    "href": "slides-02.html#recap-supervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Recap: Supervised learning",
    "text": "Recap: Supervised learning\n\nWe wish to find a model function \\(f\\) that relates \\(X\\) to \\(y\\).\nWe use the model function to predict targets of new examples.\n\n\n\n\n\n\nIn the first part of this course, we‚Äôll focus on supervised machine learning."
  },
  {
    "objectID": "slides-02.html#unsupervised-learning",
    "href": "slides-02.html#unsupervised-learning",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\n\nTraining data consists of observations \\(X\\) without any corresponding targets.\nUnsupervised learning could be used to group similar things together in \\(X\\) or to find underlying structure in the data."
  },
  {
    "objectID": "slides-02.html#clicker-2.2-supervised-vs-unsupervised",
    "href": "slides-02.html#clicker-2.2-supervised-vs-unsupervised",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Clicker 2.2: Supervised vs unsupervised",
    "text": "Clicker 2.2: Supervised vs unsupervised\nParticipate using Agora (code: agentic)\nSelect all of the following statements which are examples of supervised machine learning\n\n\nFinding groups of similar properties in a real estate data set.\n\n\nPredicting whether someone will have a heart attack or not on the basis of demographic, diet, and clinical measurement.\n\n\nGrouping articles on different topics from different news sources (something like the Google News app).\n\n\nDetecting credit card fraud based on examples of fraudulent and non-fraudulent transactions.\n\n\nGiven some measure of employee performance, identify the key factors which are likely to influence their performance."
  },
  {
    "objectID": "slides-02.html#clicker-2.3-classification-vs.-regression",
    "href": "slides-02.html#clicker-2.3-classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Clicker 2.3: Classification vs.¬†Regression",
    "text": "Clicker 2.3: Classification vs.¬†Regression\nParticipate using Agora (code: agentic)\nSelect all of the following statements which are examples of regression problems\n\n\nPredicting the price of a house based on features such as number of bedrooms and the year built.\n\n\nPredicting if a house will sell or not based on features like the price of the house, number of rooms, etc.\n\n\nPredicting percentage grade in CPSC 330 based on past grades.\n\n\nPredicting whether you should bicycle tomorrow or not based on the weather forecast.\n\n\nPredicting appropriate thermostat temperature based on the wind speed and the number of people in a room."
  },
  {
    "objectID": "slides-02.html#todays-focus",
    "href": "slides-02.html#todays-focus",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Today‚Äôs focus",
    "text": "Today‚Äôs focus\n\nML Terminology\nUsing sklearn to build a simple supervised ML model\nIntuition of Decision Trees"
  },
  {
    "objectID": "slides-02.html#framework",
    "href": "slides-02.html#framework",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Framework",
    "text": "Framework\n\nThere are many frameworks to do do machine learning.\nWe‚Äôll mainly be using scikit-learn framework."
  },
  {
    "objectID": "slides-02.html#running-example",
    "href": "slides-02.html#running-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Running example",
    "text": "Running example\nImagine you‚Äôre in the fortunate situation where, after graduating, you have a few job offers and need to decide which one to choose. You want to pick the job that will likely make you the happiest. To help with your decision, you collect data from like-minded people.\n\nCan you think of relevant features for this problem?"
  },
  {
    "objectID": "slides-02.html#toy-job-happinees-dataset",
    "href": "slides-02.html#toy-job-happinees-dataset",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Toy job happinees dataset",
    "text": "Toy job happinees dataset\nHere are the first few rows of a toy dataset.\n\ntoy_happiness_df = pd.read_csv(DATA_DIR + 'toy_job_happiness.csv')\ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides-02.html#features-target-example",
    "href": "slides-02.html#features-target-example",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Features, target, example",
    "text": "Features, target, example\n\nTerminologyData\n\n\n\nWhat are the features \\(X\\)?\n\nfeatures = inputs = predictors = explanatory variables = regressors = independent variables = covariates\n\nWhat‚Äôs the target \\(y\\)?\n\ntarget = output = outcome = response variable = dependent variable = labels\n\nWhat is an example?\n\n\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides-02.html#classification-vs.-regression",
    "href": "slides-02.html#classification-vs.-regression",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Classification vs.¬†Regression",
    "text": "Classification vs.¬†Regression\n\nIs this a classification problem or a regression problem?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides-02.html#optional-inference-vs.-prediction",
    "href": "slides-02.html#optional-inference-vs.-prediction",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "(Optional) Inference vs.¬†Prediction",
    "text": "(Optional) Inference vs.¬†Prediction\n\n\n\nInference asks: Why does something happen?\n\nGoal: understand and quantify the relationship between variables\n\nOften involves estimating model parameters and testing hypotheses\n\nExample: Which factors influence happiness, and by how much?\n\n\n\n\nPrediction asks: What will happen?\n\nGoal: accurately predict the target without needing to fully explain the relationships\n\nExample: Will you be happy in a particular job?\n\n\n\nOf course these goals are related, and in many situations we need both."
  },
  {
    "objectID": "slides-02.html#training",
    "href": "slides-02.html#training",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Training",
    "text": "Training\n\nIn supervised ML, the goal is to learn a function that maps input features (\\(X\\)) to a target (\\(y\\)).\nThe relationship between \\(X\\) and \\(y\\) is often complex, making it difficult to define mathematically.\nWe use algorithms to approximate this complex relationship between \\(X\\) and \\(y\\).\nTraining is the process of applying an algorithm to learn the best function (or model) that maps \\(X\\) to \\(y\\).\nIn this course, I‚Äôll help you develop an intuition for how these models work and demonstrate how to use them in a machine learning pipeline."
  },
  {
    "objectID": "slides-02.html#error-and-accuracy",
    "href": "slides-02.html#error-and-accuracy",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Error and accuracy",
    "text": "Error and accuracy\n\nMachine learning models are not perfect‚Äîthey will make mistakes.\n\nTo judge whether a model is useful, we need to track its performance.\n\nFor classification problems, the most common (and default in sklearn) metric is accuracy:\n\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of examples}}\n\\]"
  },
  {
    "objectID": "slides-02.html#separating-x-and-y",
    "href": "slides-02.html#separating-x-and-y",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Separating \\(X\\) and \\(y\\)",
    "text": "Separating \\(X\\) and \\(y\\)\n\nIn order to train a model we need to separate \\(X\\) and \\(y\\) from the dataframe.\n\n\nX = toy_happiness_df.drop(columns=[\"happy?\"]) # Extract the feature set by removing the target column \"happy?\"\ny = toy_happiness_df[\"happy?\"] # Extract the target variable \"happy?\""
  },
  {
    "objectID": "slides-02.html#baseline",
    "href": "slides-02.html#baseline",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Baseline",
    "text": "Baseline\n\nLet‚Äôs try a simplest algorithm of predicting the most popular target!\n\n\nfrom sklearn.dummy import DummyClassifier\nmodel = DummyClassifier(strategy=\"most_frequent\") # Initialize the DummyClassifier to always predict the most frequent class\nmodel.fit(X, y) # Train the model on the feature set X and target variable y\ntoy_happiness_df['dummy_predictions'] = model.predict(X) # Add the predicted values as a new column in the dataframe\ntoy_happiness_df\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\ndummy_predictions\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\nHappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\nHappy\n\n\n2\n1\n80000\n1\n0\nHappy\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy\nHappy"
  },
  {
    "objectID": "slides-02.html#which-question-is-more-effective",
    "href": "slides-02.html#which-question-is-more-effective",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Which question is more effective?",
    "text": "Which question is more effective?\n\nVisualData\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides-02.html#what-are-we-trying-to-learn",
    "href": "slides-02.html#what-are-we-trying-to-learn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "What are we trying to learn?",
    "text": "What are we trying to learn?\n\nWe want to learn which questions to ask and in what order.\nHow many possible questions could we ask with these features?\n\n\n\n\n\n\n\n\n\n\nsupportive_colleagues\nsalary\nfree_coffee\nboss_vegan\nhappy?\n\n\n\n\n0\n0\n70000\n0\n1\nUnhappy\n\n\n1\n1\n60000\n0\n0\nUnhappy\n\n\n2\n1\n80000\n1\n0\nHappy\n\n\n3\n1\n110000\n0\n1\nHappy\n\n\n4\n1\n120000\n1\n0\nHappy\n\n\n5\n1\n150000\n1\n1\nHappy\n\n\n6\n0\n150000\n1\n0\nUnhappy"
  },
  {
    "objectID": "slides-02.html#decision-tree-training-high-level",
    "href": "slides-02.html#decision-tree-training-high-level",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision tree Training (high level)",
    "text": "Decision tree Training (high level)\n\nTraining a decision tree is a search process: we look for the ‚Äúbest‚Äù tree among many possible ones.\nThere are different algorithms for learning trees. Check this out.\nAt each step, we evaluate candidate questions using measures such as:\n\nInformation gain\nGini index\n\nThe goal is to split the data into groups with greater certainty (more homogeneous outcomes)."
  },
  {
    "objectID": "slides-02.html#decision-tree-with-sklearn",
    "href": "slides-02.html#decision-tree-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision tree with sklearn",
    "text": "Decision tree with sklearn\nLet‚Äôs train a simple decision tree on our toy dataset using sklearn\n\nfrom sklearn.tree import DecisionTreeClassifier # import the classifier\nfrom sklearn.tree import plot_tree\n\nmodel = DecisionTreeClassifier(max_depth=2, random_state=1) # Create a class object\nmodel.fit(X, y)\nplot_tree(model, filled=True, feature_names = X.columns, class_names=[\"Happy\", \"Unhappy\"], impurity = False, fontsize=12);"
  },
  {
    "objectID": "slides-02.html#prediction",
    "href": "slides-02.html#prediction",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction",
    "text": "Prediction\n\nGiven a new example, how does a decision tree predict the class of this example?\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, coffee_machine = 0, vegan_boss = 1,"
  },
  {
    "objectID": "slides-02.html#prediction-with-sklearn",
    "href": "slides-02.html#prediction-with-sklearn",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Prediction with sklearn",
    "text": "Prediction with sklearn\n\nWhat would be the prediction for the example below using the tree above?\n\nsupportive_colleagues = 1, salary = 60000, free_coffee = 0, vegan_boss = 1,\n\n\n\ntest_example = [[1, 60000, 0, 1]]\nprint(\"Model prediction: \", model.predict(test_example))\nplot_tree(model, filled=True, feature_names = X.columns, class_names = [\"Happy\", \"Unhappy\"], impurity = False, fontsize=9);\n\nModel prediction:  ['Unhappy']"
  },
  {
    "objectID": "slides-02.html#parameters-vs.-hyperparameters",
    "href": "slides-02.html#parameters-vs.-hyperparameters",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Parameters vs.¬†Hyperparameters",
    "text": "Parameters vs.¬†Hyperparameters\n\nParameters\n\nThe questions (features and thresholds) used to split the data at each node.\nExample: salary &lt;= 75000 at the root node\n\n\nHyperparameters\n\nSettings that control tree growth, like max_depth, which limits how deep the tree can go."
  },
  {
    "objectID": "slides-02.html#decision-boundary",
    "href": "slides-02.html#decision-boundary",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary",
    "text": "Decision boundary\n\nA decision boundary is the line, curve, or surface that separates classes.\nPoints on one side \\(\\rightarrow\\) Model predicts Class Happy\n\nPoints on the other side \\(\\rightarrow\\) Model predicts Class Unhappy"
  },
  {
    "objectID": "slides-02.html#decision-boundary-with-max_depth1",
    "href": "slides-02.html#decision-boundary-with-max_depth1",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=1",
    "text": "Decision boundary with max_depth=1"
  },
  {
    "objectID": "slides-02.html#decision-boundary-with-max_depth2",
    "href": "slides-02.html#decision-boundary-with-max_depth2",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Decision boundary with max_depth=2",
    "text": "Decision boundary with max_depth=2"
  },
  {
    "objectID": "slides-02.html#clicker-2.4-baselines-and-decision-trees",
    "href": "slides-02.html#clicker-2.4-baselines-and-decision-trees",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Clicker 2.4: Baselines and Decision trees",
    "text": "Clicker 2.4: Baselines and Decision trees\niParticipate using Agora (code: agentic) https://join.iclicker.com/FZMQ\nSelect all of the following statements which are TRUE.\n\n\nChange in features (i.e., binarizing features above) would change DummyClassifier predictions.\n\n\npredict takes only X as argument whereas fit and score take both X and y as arguments.\n\n\nFor the decision tree algorithm to work, the feature values must be binary.\n\n\nThe prediction in a decision tree works by routing the example from the root to the leaf."
  },
  {
    "objectID": "slides-02.html#summary",
    "href": "slides-02.html#summary",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Summary",
    "text": "Summary\n\nTerminology\nsklearn basic steps\nDecision tree intuition"
  },
  {
    "objectID": "slides-02.html#break",
    "href": "slides-02.html#break",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Break",
    "text": "Break\nLet‚Äôs take a break!"
  },
  {
    "objectID": "slides-02.html#group-work-class-demo-live-coding",
    "href": "slides-02.html#group-work-class-demo-live-coding",
    "title": "Lecture 2: Terminology, Baselines, Decision Trees",
    "section": "Group Work: Class Demo & Live Coding",
    "text": "Group Work: Class Demo & Live Coding\nIn some of the classes, we will do a bit of live coding to get your used to practical machine learning. You are highly encouraged to follow along - we won‚Äôt usually finish everything in the demo, but it should be a significant portion that you can finish off after class.\nFor this demo, each student should click this link to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today."
  }
]