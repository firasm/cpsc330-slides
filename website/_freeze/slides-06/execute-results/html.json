{
  "hash": "8c5fb59e2da657b270f5394b5b6bf499",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lecture 6: Column transformer and text features\"\nauthor: \"Firas Moosvi (Slides adapted from Varada Kolhatkar)\"\ndescription: \"Column transformer and introduction to text features\"\ndescription-short: \"Preprocessing and sklearn pipelines\"\nformat:\n  revealjs:\n    embed-resources: true\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    resources:\n      - data/\n      - img/  \n---\n\n\n## Announcements \n\n- Lecture recordings for the first two weeks have been made available - See Piazza.\n- My Office Hours\n- HW3 is due next week Tuesday, Oct 1st, 11:59 pm. \n  - You can work in pairs for this assignment. \n\n## Quick Correction on Exercise 5.3\n\n> I accidentally said *only D is true*, but **B** is also true!\n\nSelect all of the following statements which are TRUE.\n\n- (A) You can have scaling of numeric features, one-hot encoding of categorical features, and scikit-learn estimator within a single pipeline.\n- (B) Once you have a scikit-learn pipeline object with an estimator as the last step, you can call fit, predict, and score on it.\n- (C) You can carry out data splitting within scikit-learn pipeline.\n- (D) We have to be careful of the order we put each transformation and model in a pipeline.\n\n\n\n# Recap: Preprocessing mistakes\n\n## Data \n\n::: {#c1b8ea3e .cell execution_count=2}\n``` {.python .cell-code}\nX, y = make_blobs(n_samples=100, centers=3, random_state=12, cluster_std=5) # make synthetic data\nX_train_toy, X_test_toy, y_train_toy, y_test_toy = train_test_split(\n    X, y, random_state=5, test_size=0.4) # split it into training and test sets\n# Visualize the training data\nplt.scatter(X_train_toy[:, 0], X_train_toy[:, 1], label=\"Training set\", s=60)\nplt.scatter(\n    X_test_toy[:, 0], X_test_toy[:, 1], color=mglearn.cm2(1), label=\"Test set\", s=60\n)\nplt.legend(loc=\"upper right\")\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-06_files/figure-revealjs/cell-3-output-1.png){width=804 height=421}\n:::\n:::\n\n\n## ❌ Bad ML 1\n- What's wrong with the approach below? \n\n::: {#7d5b8130 .cell execution_count=3}\n``` {.python .cell-code}\nscaler = StandardScaler() # Creating a scalert object \nscaler.fit(X_train_toy) # Calling fit on the training data \ntrain_scaled = scaler.transform(\n    X_train_toy\n)  # Transforming the training data using the scaler fit on training data\n\nscaler = StandardScaler()  # Creating a separate object for scaling test data\nscaler.fit(X_test_toy)  # Calling fit on the test data\ntest_scaled = scaler.transform(\n    X_test_toy\n)  # Transforming the test data using the scaler fit on test data\n\nknn = KNeighborsClassifier()\nknn.fit(train_scaled, y_train_toy)\nprint(f\"Training score: {knn.score(train_scaled, y_train_toy):.2f}\")\nprint(f\"Test score: {knn.score(test_scaled, y_test_toy):.2f}\") # misleading scores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining score: 0.63\nTest score: 0.60\n```\n:::\n:::\n\n\n## Scaling train and test data separately\n\n::: {#614b2ccd .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](slides-06_files/figure-revealjs/cell-5-output-1.png){width=1176 height=373}\n:::\n:::\n\n\n## ❌ Bad ML 2 \n- What's wrong with the approach below? \n\n::: {#98c34649 .cell execution_count=5}\n``` {.python .cell-code}\n# join the train and test sets back together\nXX = np.vstack((X_train_toy, X_test_toy))\n\nscaler = StandardScaler()\nscaler.fit(XX)\nXX_scaled = scaler.transform(XX)\n\nXX_train = XX_scaled[:X_train_toy.shape[0]]\nXX_test = XX_scaled[X_train_toy.shape[0]:]\n\nknn = KNeighborsClassifier()\nknn.fit(XX_train, y_train_toy)\nprint(f\"Training score: {knn.score(XX_train, y_train_toy):.2f}\")  # Misleading score\nprint(f\"Test score: {knn.score(XX_test, y_test_toy):.2f}\")  # Misleading score\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining score: 0.63\nTest score: 0.55\n```\n:::\n:::\n\n\n## ❌ Bad ML 3\n\n- What's wrong with the approach below? \n\n::: {#8a9dd656 .cell execution_count=6}\n``` {.python .cell-code}\nknn = KNeighborsClassifier()\n\nscaler = StandardScaler()\nscaler.fit(X_train_toy)\nX_train_scaled = scaler.transform(X_train_toy)\nX_test_scaled = scaler.transform(X_test_toy)\ncross_val_score(knn, X_train_scaled, y_train_toy)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([0.25      , 0.5       , 0.58333333, 0.58333333, 0.41666667])\n```\n:::\n:::\n\n\n::: {.notes}\nSpeaker notes go here.\n:::\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 500px;\"}\n## Improper preprocessing\n\n::: {#aae6e249 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-06_files/figure-revealjs/cell-8-output-1.png){width=1135 height=801}\n:::\n:::\n\n\n:::\n\n::: {.scroll-container style=\"overflow-y: scroll; height: 500px;\"}\n## Proper preprocessing\n\n::: {#1e350cd4 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-06_files/figure-revealjs/cell-9-output-1.png){width=1135 height=653}\n:::\n:::\n\n\n:::\n\n## Recap: `sklearn` Pipelines\n\n- Pipeline is a way to chain multiple steps (e.g., preprocessing + model fitting) into a single workflow.\n- Simplify the code and improves readability.\n- Reduce the risk of data leakage by ensuring proper transformation of the training and test sets.\n- Automatically apply transformations in sequence.\n- **Example:**\n  - Chaining a `StandardScaler` with a `KNeighborsClassifier` model.\n\n::: {#243fe8bd .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.pipeline import make_pipeline\n\npipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\n# Correct way to do cross validation without breaking the golden rule. \ncross_val_score(pipe_knn, X_train_toy, y_train_toy) \n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([0.25      , 0.5       , 0.5       , 0.58333333, 0.41666667])\n```\n:::\n:::\n\n\n## Group Work: Class Demo & Live Coding\n\nFor this demo, each student should [click this link](https://github.com/new?template_name=lecture6_demo&template_owner=ubc-cpsc330) to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today.\n\n<br>\n<br>\n<br>\n\nIf you really don't want to create a repo,\n\n- Navigate to the `cpsc330-2024W1` repo\n- run `git pull` to pull the latest files in the course repo\n- Look for the demo file here: `lectures/102-Firas-lectures/class_demos/`.\n\n## `sklearn`'s `ColumnTransformer` \n\n- Use ColumnTransformer to build all our transformations together into one object \n\n![](img/column-transformer.png)\n\n- Use a column transformer with sklearn pipelines. \n\n## (iClicker) Exercise 6.1\n**iClicker cloud join link: [https://join.iclicker.com/VYFJ](https://join.iclicker.com/VYFJ)**\n\n**Select all of the following statements which are TRUE.**\n\n- (A) You could carry out cross-validation by passing a `ColumnTransformer` object to `cross_validate`.\n- (B) After applying column transformer, the order of the columns in the transformed data has to be the same as the order of the columns in the original data.\n- (C) After applying a column transformer, the transformed data is always going to be of different shape than the original data.\n- (D) When you call `fit_transform` on a `ColumnTransformer` object, you get a numpy ndarray.\n\n::: {.notes}\niClicker 6.1\n\nA. False, column transfer to pipeline. There is no estimator attached to ColumnTransfer\n\nB. False, numeric, binary, ordinal, categorical etc…Can also use this to remove columns\n\nC. False, often it will be different, but not always\n\nD. True, just an ndarray, not a data frame\n:::\n\n",
    "supporting": [
      "slides-06_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}