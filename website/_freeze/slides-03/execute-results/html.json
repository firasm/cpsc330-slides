{
  "hash": "546fb6f8db8878fc98e28298aa84cc2e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Lecture 3: ML fundamentals'\nauthor: \"Firas Moosvi (Slides adapted from Varada Kolhatkar)\"\ndescription: Supervised Machine Learning Fundamentals\ndescription-short: 'generalization, data splitting, overfitting, underfitting, the fundamental tradeoff, the golden rule'\nformat:\n  revealjs:\n    slide-number: true\n    smaller: true\n    center: true\n    logo: img/UBC-CS-logo.png\n    theme: [night, slides.scss]\n    resources:\n      - data/\n      - img/\n---\n\n## Announcements\n\n- My weekly office hours will be announced soon!\n\n- Homework 2 (hw2) was released on Monday, it is due Jan 19, 11:59 pm\n  - You are welcome to broadly discuss it with your classmates but final answers and submissions must be your own.\n  - Group submissions are not allowed for this assignment.\n- Advice on keeping up with the material\n  - Practice!\n  - Make sure you run the lecture notes on your laptop and experiment with the code. \n  - Start early on homework assignments.\n\n- Last day to drop without a W standing is this Friday: **January 16, 2026**\n\n## Dropping lowest homework (Update)\n\n- CPSC 330 has 9 homework assignments that are all an integral part of your learning\n- To account for illnesses, other commitments, and to preserve your mental health, there has long been a policy of dropping your lowest HW score.\n- After some analysis from the data from previous terms (Learning Analytics!), there is a slight modification to this policy:\n\n> **With the exception of HW5,** we will drop your lowest homework grade - all students must complete HW5.\n\n- This is to encourage all students to complete HW5! It's important!\n\n## Recap\n\n- Importance of generalization in supervised machine learning\n- Data splitting as a way to approximate generalization error\n- Train, test, validation, deployment data\n- Overfitting, underfitting, the fundamental tradeoff, and the golden rule.\n- Cross-validation\n\n## Finish up demo from last class\n\nFor this demo, each student should [click this link](https://github.com/new?template_name=lecture02_demo&template_owner=ubc-cpsc330) to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today.\n\n## Recap\n\nA typical sequence of steps to train supervised machine learning models\n\n- training the model on the train split\n- tuning hyperparamters using the validation split\n- checking the generalization performance on the test split\n\n## Clicker 3.1\n\nParticipate using [Agora](https://agora.students.cs.ubc.ca) (code: agentic)\n\n**Select all of the following statements which are TRUE.**\n\n- (A) A decision tree model with no depth (the default `max_depth` in `sklearn`) is likely to perform very well on the deployment data.\n- (B) Data splitting helps us assess how well our model would generalize.\n- (C) Deployment data is scored only once.\n- (D) Validation data could be used for hyperparameter optimization.\n- (E) It’s recommended that data be shuffled before splitting it into train and test sets.\n\n\n## Clicker 3.2\n\nParticipate using [Agora](https://agora.students.cs.ubc.ca) (code: agentic)\n\n**Select all of the following statements which are TRUE.**\n\n- (A) $k$-fold cross-validation calls fit $k$ times\n- (B) We use cross-validation to get a more robust estimate of model performance.\n- (C) If the mean train accuracy is much higher than the mean cross-validation accuracy it's likely to be a case of overfitting.\n- (D) The fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n- (E) A decision stump on a complicated classification problem is likely to underfit.\n\n## Recap from videos \n- Why do we split the data? What are train/valid/test splits? \n- What are the benefits of cross-validation?\n- What is underfitting and overfitting? \n- What’s the fundamental trade-off in supervised machine learning?\n- What is the golden rule of machine learning?\n\n## Summary of train, validation, test, and deployment data \n\n|         | `fit` | `score` | `predict` |\n|----------|-------|---------|-----------|\n| Train    | ✔️      | ✔️      | ✔️         |\n| Validation |      | ✔️      | ✔️         |\n| Test    |       |  once   | once         |\n| Deployment    |       |       | ✔️         |\n\n## Cross validation\n\n![](img/cross-validation.png){fig-align=\"center\"}\n\n## Cross validation\n\n::: {#dc86d768 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](slides-03_files/figure-revealjs/cell-2-output-1.png){width=1161 height=228}\n:::\n:::\n\n\n## Overfitting and underfitting \n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![](img/underfit-overfit-google-developer.png)\n\n[Source](https://developers.google.com/machine-learning/crash-course/overfitting/overfitting)\n:::\n\n::: {.column width=\"40%\"}\n- An **overfit model** matches the training set so closely that it fails to make correct predictions on new unseen data.  \n- An **underfit model** is too simple and does not even make good predictions on the training data \n\n:::\n::::\n\n## The fundamental tradeoff\n\nAs you increase the model complexity, training score tends to go up and the gap between train and validation scores tends to go up.  \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n![](img/malp_0201.png){fig-align=\"center\"}\n:::\n\n::: {.column width=\"50%\"}\n- Underfitting: Both accuracies rise\n- Sweet spot: Validation accuracy peaks\n- Overfitting: Training $\\uparrow$, Validation $\\downarrow$\n- Tradeoff: Balance complexity to avoid both\n:::\n\n::::\n\n## The golden rule\n- Although our primary concern is the model's performance on the test data, this data should not influence the training process in any way.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n![](img/golden_rule_analogy.png){fig-align=\"center\"}\nSource: Image generated by ChatGPT 5\n:::\n\n::: {.column width=\"50%\"}\n- **Test data = final exam**  \n- You can practice all you want with training/validation data\n- But **never peek** at the test set before evaluation\n- Otherwise, it's like sneaking answers before the exam $\\rightarrow$ **not a real assessment of your learning**.  \n:::\n\n::::\n\n## Additional Resource on Cross Validation\n\n::: {#f09f7e4c .cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n\n        <iframe\n            width=\"1000\"\n            height=\"650\"\n            src=\"https://mlu-explain.github.io/cross-validation/\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        \n```\n:::\n:::\n\n\nReference: [MLU-Explain - Cross Validation](https://mlu-explain.github.io/cross-validation/)\n\n## Break\n\nLet's take a break!\n\n![](img/eva-coffee.png){fig-align=\"center\"}\n\n## Group Work: Class Demo & Live Coding\n\nFor this demo, each student should [click this link](https://github.com/new?template_name=lecture03_demo&template_owner=ubc-cpsc330) to create a new repo in their accounts, then clone that repo locally to follow along with the demo from today.\n\n<br>\n<br>\n<br>\n\n",
    "supporting": [
      "slides-03_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}