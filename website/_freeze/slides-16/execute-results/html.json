{
  "hash": "0c94308baea81039ffe88387c2b6a6ee",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'CPSC 330 Lecture 16: DBSCAN, Hierarchical Clustering'\ndescription: \"Unsupervised paradigm, motivation and potential applications of clustering, K-Means algorithm, pros and cons of K-Means, the Elbow plot and Silhouette plots for a given dataset,  importance of input data representation in clustering.\"\nformat:\n    revealjs:\n        html-math-method: plain\n        slide-number: true\n        slide-level: 2\n        theme:\n          - slides.scss\n        center: true\n        logo: img/UBC-CS-logo.png\n        resources:\n          - data/\n          - img/\n\neditor:\n  render-on-save: true\n---\n\n\n## Announcements \n\n- HW6 is due next week Wednesday. \n  - Computationally intensive (welcome to real-life ML!)\n  - heads up: you will need to install many packages  (welcome to real-life ML!)\n\n::: {#e4a1326c .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-2-output-1.png){width=254 height=444}\n:::\n:::\n\n\n## Super cool Demo! \n\n[You can download and checkout the Demo here](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_15-k-means.ipynb).\n\nAll credit to Dr. Varada Kolhatkar for putting this together!\n\n## Recap: iClicker Exercise 15.3 {.smaller}\n\nSelect all of the following statements which are **True** \n\n- (A) If you train K-Means with `n_clusters`= the number of examples, the inertia value will be 0. \n- (B) The elbow plot shows the tradeoff between within cluster distance and the number of clusters.\n- (C) Unlike the Elbow method, the Silhouette method is not dependent on the notion of cluster centers.\n- (D) The elbow plot is not a reliable method to obtain the optimal number of clusters in all cases. \n- (E) The Silhouette scores ranges between -1 and 1 where higher scores indicates better cluster assignments.  \n\n## iClicker Exercise 16.1 \n\n**Select all of the following statements which are TRUE.**\n\n- (A) Similar to K-nearest neighbours, K-Means is a non parametric model.\n- (B) The meaning of $K$ in K-nearest neighbours and K-Means clustering is very similar. \n- (C) Scaling of input features is crucial in clustering.  \n- (D) In clustering, it's almost always a good idea to find equal-sized clusters. \n\n# Limitations of K-means\n\n## Shape of clusters\n- Good for spherical clusters of more or less equal sizes \n![](img/kmeans_boundaries.png)\n\n## K-Means: failure case 1\n\n- K-Means performs poorly if the clusters have more complex shapes (e.g., two moons data below). \n\n::: {#f42079ec .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-3-output-1.png){width=826 height=375}\n:::\n:::\n\n\n## K-Means: failure case 2\n\n- Again, K-Means is unable to capture complex cluster shapes. \n\n::: {#00157024 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-4-output-1.png){width=818 height=375}\n:::\n:::\n\n\n## K-Means: failure case 3\n\n- It assumes that all directions are equally important for each cluster and fails to identify non-spherical clusters. \n\n::: {#c090ea9b .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-5-output-1.png){width=807 height=375}\n:::\n:::\n\n\n# Can we do better? \n\n# DBSCAN\n\n- **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise\n- A density-based clustering algorithm\n\n## DBSCAN\n\n::: {#1c547144 .cell execution_count=5}\n``` {.python .cell-code}\nX, y = make_moons(n_samples=200, noise=0.08, random_state=42)\ndbscan = DBSCAN(eps=0.2)\ndbscan.fit(X)\nplot_original_clustered(X, dbscan, dbscan.labels_)\n```\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-6-output-1.png){width=974 height=375}\n:::\n:::\n\n\n## How does it work?\n![](img/DBSCAN_search.gif)\n\n## DBSCAN Analogy\n\nConsider DBSCAN in a social context: \n\n- Social butterflies (ü¶ã): Core points\n- Friends of social butterflies who are not social butterflies: Border points\n- Lone wolves (üê∫): Noise points  \n\n## Two main hyperparameters\n- `eps`: determines what it means for points to be \"close\"\n- `min_samples`: determines the number of **neighboring points** we require to consider in order for a point to be part of a cluster\n\n## DBSCAN: failure cases\n\n- Let's consider this dataset with three clusters of varying densities.  \n- K-Means performs better compared to DBSCAN. But it has the benefit of knowing the value of $K$ in advance. \n\n::: {#f8be5d75 .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-7-output-2.png){width=1390 height=375}\n:::\n:::\n\n\n## Break\n\nLet's take a 10-min break!\n\n## Dendrogram\n\nDefinition: visual representation of a tree, in particular, the hierarchical representation of data...\n\n![](img/reading_dendrogram)\n\n[Source](https://www.displayr.com/what-is-dendrogram/)\n\n## Example: Languages\n\n![](img/dendrogram_languages.jpg)\n\n[Source](https://pmc.ncbi.nlm.nih.gov/articles/PMC5325856/)\n\n## Hierarchical clustering \n\n::: {#09f504c3 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-8-output-1.png){width=1073 height=378}\n:::\n:::\n\n\n## Flat clusters\n\n- This is good but how can we get cluster labels from a dendrogram? \n- We can bring the clustering to a \"flat\" format use `fcluster`\n\n::: {#6ac87c37 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](slides-16_files/figure-revealjs/cell-9-output-1.png){width=948 height=378}\n:::\n:::\n\n\n## Linkage criteria {.smaller}\n- When we create a dendrogram, we need to calculate distance between clusters. How do we measure distances between clusters? \n- The **linkage criteria** determines how to find similarity between clusters:\n- Some example linkage criteria are: \n    - Single linkage $\\rightarrow$ smallest minimal distance, leads to loose clusters\n    - Complete linkage $\\rightarrow$ smallest maximum distance, leads to tight clusters \n    - Average linkage $\\rightarrow$ smallest average distance between all pairs of points in the clusters\n    - Ward linkage $\\rightarrow$ smallest increase in within-cluster variance, leads to equally sized clusters\n\n## Activity\n\n- Fill in the table below\n\n| **Clustering Method**  | **KMeans**                                           | **DBSCAN**                                          | **Hierarchical Clustering**                            |\n|------------------------|------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------|\n| **Approach**           | | |\n| **Hyperparameters**    | | |\n| **Shape of clusters**  | | |\n| **Handling noise**     | | | \n| **Examples**           | | |\n\n\n<!-- \n# [Class demo](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/lectures/102-Varada-lectures/class_demos/demo_16-dbscan-hierarchical.ipynb) \n-->\n\n",
    "supporting": [
      "slides-16_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}