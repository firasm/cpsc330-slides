{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"CPSC 330 Lecture 19: Introduction to deep learning and computer vision\"\n",
        "author: \"Firas Moosvi\"\n",
        "format: \n",
        "    revealjs:\n",
        "      embed-resources: true\n",
        "      slide-number: true\n",
        "      smaller: true\n",
        "      center: true\n",
        "      logo: img/UBC-CS-logo.png\n",
        "      resources:\n",
        "        - data/\n",
        "        - img/        \n",
        "---"
      ],
      "id": "a4b98488"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mglearn\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys, os\n",
        "sys.path.append(os.path.join(os.path.abspath(\".\"), \"code\"))\n",
        "from deep_learning_code import *\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline"
      ],
      "id": "2cb3def6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Announcements\n",
        "\n",
        "- HW7 was due yesterday \n",
        "- HW8 has been released (due next week Monday)\n",
        "  - Almost there! Hang in there ðŸ˜Š\n",
        "- Midterm 2 grading is in progress. \n",
        "\n",
        "# iClicker 19.0\n",
        "\n",
        "iClicker cloud join link: **https://join.iclicker.com/VYFJ**\n",
        "\n",
        "**Select all of the following statements which are TRUE for you.**\n",
        "\n",
        "- (A) I found the multiple-choice questions challenging.\n",
        "- (B) The coding questions took a lot of time.\n",
        "- (C) I didn't like the format of the midterm.\n",
        "- (D) I appreciated the mix of coding, conceptual, and multiple-choice questions.\n",
        "- (E) I felt the midterm was a good reflection of what we cover in the lectures and homework assignments.\n",
        "\n",
        "## iClicker 19.1\n",
        "\n",
        "iClicker cloud join link: **https://join.iclicker.com/VYFJ**\n",
        "\n",
        "**Select all of the following statements which are TRUE.**\n",
        "\n",
        "- (A) It's possible to use word2vec embedding representations for text classification instead of bag-of-words representation. \n",
        "- (B) The topic model approach we used in the last lecture, Latent Dirichlet Allocation (LDA), is an unsupervised approach. \n",
        "- (C) In an LDA topic model, the same word can be associated with two different topics with high probability.\n",
        "- (D) In an LDA topic model, a document is a mixture of multiple topics. \n",
        "- (E) If I train a topic model on a large collection of news articles with K = 10, I would get 10 topic labels (e.g., sports, culture, politics, finance) as output. \n",
        "\n",
        "## Multiclass classification\n",
        "\n",
        "- So far we have been talking about binary classification \n",
        "- Can we use these classifiers when there are more than two classes? \n",
        "    - [\"ImageNet\" computer vision competition](http://www.image-net.org/challenges/LSVRC/), for example, has 1000 classes \n",
        "- Can we use decision trees or KNNs for multi-class classification?\n",
        "- What about logistic regression?\n",
        "\n",
        "# Multinomial logistic regression\n",
        "\n",
        "## Softmax Function for Probabilities\n",
        "\n",
        "Given an input, the probability that it belongs to class $j \\in \\{1, 2, \\dots, K\\}$ is calculated using the **softmax function**:\n",
        "\n",
        "$P(y = j \\mid x_i) = \\frac{e^{w_j^\\top x_i + b_j}}{\\sum_{k=1}^{K} e^{w_k^\\top x_i + b_k}}$\n",
        "\n",
        "- $x_i$ is the $i^{th}$ example \n",
        "- $w_j$ is the weight vector for class $j$.\n",
        "- $b_j$ is the bias term for class $j$.\n",
        "- $K$ is the total number of classes.\n",
        "\n",
        "\n",
        "## Making Predictions\n",
        "\n",
        "1. **Compute Probabilities**:  \n",
        "   For each class $j$, compute the probability $P(y = j \\mid x_i)$ using the softmax function.\n",
        "\n",
        "2. **Select the Class with the Highest Probability**:  \n",
        "   The predicted class \\(\\hat{y}\\) is:  \n",
        "   $\\hat{y} = \\arg \\max_{j \\in \\{1, \\dots, K\\}} P(y = j \\mid x_i)$\n",
        "\n",
        "\n",
        "\n",
        "## Binary vs multinomial logistic regression \n",
        "\n",
        "|   **Aspect**                       | **Binary Logistic Regression**              | **Multinomial Logistic Regression**  |\n",
        "|--------------------------|---------------------------------------------|--------------------------------------|\n",
        "| **Target variable**      | 2 classes (binary)                          | More than 2 classes (multi-class)    |\n",
        "| **Getting probabilities**  | Sigmoid                                   | Softmax                              |\n",
        "| parameters               | $d$ weights, one per feature and the bias term | $d$ weights and a bias term per class |  \n",
        "| **Output**               | Single probability                          | Probability distribution over classes |\n",
        "| **Use case**             | Binary classification (e.g., spam detection) | Multi-class classification (e.g., image classification) |\n",
        "\n",
        "\n",
        "## Image classification\n",
        "\\\n",
        "\n",
        "Have you used search in Google Photos? You can search for \"my photos of cat\" and it will retrieve photos from your libraries containing cats.\n",
        "This can be done using **image classification**, which is treated as a supervised learning problem, where we define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos.\n",
        "\n",
        "## Image classification\n",
        "\\\n",
        "\n",
        "Image classification is not an easy problem because of the variations in the location of the object, lighting, background, camera angle, camera focus etc.\n",
        "\n",
        "![](img/cat_variation.png)\n",
        "<!-- [Source](https://developers.google.com/machine-learning/practica/image-classification) -->\n",
        "\n",
        "## Neural networks\n",
        "\\\n",
        "\n",
        "- Neural networks are perfect for these types of problems where local structures are important. \n",
        "- A significant advancement in image classification was the application of **convolutional neural networks** (ConvNets or CNNs) to this problem. \n",
        "  - [ImageNet Classification with Deep Convolutional\n",
        "Neural Networks](https://dl.acm.org/doi/10.1145/3065386)\n",
        "  - Achieved a winning test error rate of 15.3%, compared to 26.2% achieved by the second-best entry in the ILSVRC-2012 competition. \n",
        "- Let's go over the basics of a neural network.\n",
        "\n",
        "## Introduction to neural networks\n",
        "\\\n",
        "\n",
        "- Neural networks can be viewed a generalization of linear models where we apply a series of transformations.\n",
        "- Here is graphical representation of a logistic regression model.\n",
        "- We have 4 features: x[0], x[1], x[2], x[3]\n"
      ],
      "id": "fcfd83ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mglearn\n",
        "\n",
        "mglearn.plots.plot_logistic_regression_graph()"
      ],
      "id": "4edc7c44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding a layer of transformations \n",
        "\\\n",
        "\n",
        "- Below we are adding one \"layer\" of transformations in between features and the target. \n",
        "- We are repeating the the process of computing the weighted sum multiple times.  \n",
        "- The **hidden units** (e.g., h[1], h[2], ...) represent the intermediate processing steps. \n"
      ],
      "id": "830c4ef8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mglearn.plots.plot_single_hidden_layer_graph()"
      ],
      "id": "cc6089db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One more layer of transformations \n",
        "\\\n",
        "\n",
        "- Now we are adding one more layer of transformations. \n"
      ],
      "id": "ee5a9bf7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mglearn.plots.plot_two_hidden_layer_graph()"
      ],
      "id": "7ee0573e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Neural networks \n",
        "\\\n",
        "\n",
        "- With a neural net, you specify the number of features after each transformation.\n",
        "  - In the above, it goes from 4 to 3 to 3 to 1.\n",
        "\n",
        "- To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node. \n",
        "- Neural network = neural net\n",
        "- Deep learning ~ using neural networks\n",
        "\n",
        "## Why neural networks?\n",
        "\\\n",
        "\n",
        "- They can learn very complex functions.\n",
        "  - The fundamental tradeoff is primarily controlled by the **number of layers** and **layer sizes**.\n",
        "  - More layers / bigger layers --> more complex model.\n",
        "  - You can generally get a model that will not underfit. \n",
        "\n",
        "- They work really well for structured data:\n",
        "  - 1D sequence, e.g. timeseries, language\n",
        "  - 2D image\n",
        "  - 3D image or video\n",
        "- They've had some incredible successes in the last 12 years.\n",
        "- Transfer learning (coming later today) is really useful.  \n",
        "\n",
        "## Why not neural networks?\n",
        "\\\n",
        "\n",
        "- Often they require a lot of data.\n",
        "- They require a lot of compute time, and, to be faster, specialized hardware called [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n",
        "- They have huge numbers of hyperparameters\n",
        "  - Think of each layer having hyperparameters, plus some overall hyperparameters.\n",
        "  - Being slow compounds this problem.\n",
        "- They are not interpretable.\n",
        "- I don't recommend training them on your own without further training\n",
        "- Good news\n",
        "    - You don't have to train your models from scratch in order to use them.\n",
        "    - I'll show you some ways to use neural networks without training them yourselves. \n",
        "\n",
        "## Deep learning software\n",
        "\\\n",
        "\n",
        "The current big players are:\n",
        "\n",
        "1. [PyTorch](http://pytorch.org)\n",
        "2. [TensorFlow](https://www.tensorflow.org)\n",
        "\n",
        "Both are heavily used in industry. If interested, see [comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software).\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## Introduction to computer vision\n",
        "\\\n",
        "\n",
        "- [Computer vision](https://en.wikipedia.org/wiki/Computer_vision) refers to understanding images/videos, usually using ML/AI. \n",
        "- In the last decade this field has been dominated by deep learning. We will explore **image classification** and **object detection**.\n",
        "\n",
        "## Introduction to computer vision\n",
        "\\\n",
        "\n",
        "- image classification: is this a cat or a dog?\n",
        "- object localization: where is the cat in this image?\n",
        "- object detection: What are the various objects in the image? \n",
        "- instance segmentation: What are the shapes of these various objects in the image? \n",
        "- and much more...\n",
        "\n",
        "![](img/vision-apps.jpeg)\n",
        "<!-- Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/--> \n",
        "\n",
        "\n",
        "## Pre-trained models\n",
        "\\\n",
        "\n",
        "- In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.\n",
        "- Instead, a common practice is to download a pre-trained model and fine tune it for your task. This is called **transfer learning**.\n",
        "- Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.\n",
        "- It refers to using a model already trained on one task as a starting point for learning to perform another task.\n",
        "\n",
        "## Pre-trained models out-of-the-box \n",
        "\\\n",
        "\n",
        "![](img/cnn-ex.png)\n",
        "\n",
        "<!-- Source: https://cezannec.github.io/Convolutional_Neural_Networks/ -->\n",
        "\n",
        "- Let's first apply one of these pre-trained models to our own problem right out of the box. \n",
        "\n",
        "\n",
        "## Pre-trained models out-of-the-box \n",
        "\\\n",
        "\n",
        "- We can easily download famous models using the `torchvision.models` module. All models are available with pre-trained weights (based on ImageNet's 224 x 224 images)\n",
        "- We used a pre-trained model vgg16 which is trained on the ImageNet data. \n",
        "- We preprocess the given image. \n",
        "- We get prediction from this pre-trained model on a given image along with prediction probabilities.  \n",
        "- For a given image, this model will spit out one of the 1000 classes from ImageNet. \n",
        "\n",
        "## Pre-trained models out-of-the-box {.scrollable}\n",
        "\n",
        "- Let's predict labels with associated probabilities for unseen images\n",
        "\n",
        "::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n"
      ],
      "id": "3791da57"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "images = glob.glob(\"data/test_images/*.*\")\n",
        "plt.figure(figsize=(5, 5));\n",
        "for image in images:\n",
        "    img = Image.open(image)\n",
        "    img.load()\n",
        "    \n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    df = classify_image(img)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"--------------------------------------------------------------\")"
      ],
      "id": "dd12e7ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Pre-trained models out-of-the-box \n",
        "\\\n",
        "\n",
        "- We got these predictions without \"doing the ML ourselves\".\n",
        "- We are using **pre-trained** `vgg16` model which is available in `torchvision`.\n",
        "  - `torchvision` has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.\n",
        "- Many of these models have been pre-trained on famous datasets like **ImageNet**. \n",
        "- So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification. \n",
        "\n",
        "## Pre-trained models out-of-the-box {.smaller}\n",
        "\\\n",
        "\n",
        "- Let's try some images which are unlikely to be there in ImageNet. \n",
        "- It's not doing very well here because ImageNet doesn't have proper classes for these images.\n",
        "\n",
        "::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n"
      ],
      "id": "5efb54a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predict labels with associated probabilities for unseen images\n",
        "images = glob.glob(\"data/random_img/*.*\")\n",
        "for image in images:\n",
        "    img = Image.open(image)\n",
        "    img.load()\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    df = classify_image(img)    \n",
        "    print(df.to_string(index=False))\n",
        "    print(\"--------------------------------------------------------------\")"
      ],
      "id": "e437e0a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Pre-trained models out-of-the-box\n",
        "\\\n",
        "\n",
        "- Here we are Pre-trained models out-of-the-box. \n",
        "- Can we use pre-trained models for our own classification problem with our classes? \n",
        "- Yes!! We have two options here:\n",
        "    1. Add some extra layers to the pre-trained network to suit our particular task\n",
        "    2. Pass training data through the network and save the output to use as features for training some other model\n",
        "\n",
        "\n",
        "## Pre-trained models to extract features \n",
        "\\\n",
        "\n",
        "- Let's use pre-trained models to extract features.\n",
        "- We will pass our specific data through a pre-trained network to get a feature vector for each example in the data. \n",
        "- The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network. \n",
        "- You can think of each layer a transformer applying some transformations on the input received to that later. \n",
        "\n",
        "![](img/cnn-ex.png)\n",
        "\n",
        "\n",
        "## Pre-trained models to extract features \n",
        "\\\n",
        "\n",
        "- Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest. \n",
        "- This classifier will be trained on our classes using feature representations extracted from the pre-trained models.  \n",
        "- Let's try this out. \n",
        "- It's better to train such models with GPU. Since our dataset is quite small, we won't have problems running it on a CPU. \n",
        "\n",
        "## Pre-trained models to extract features \n",
        "\\\n",
        "\n",
        "Let's look at some sample images in the dataset. \n",
        "\n",
        "::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n"
      ],
      "id": "779b4fbd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    data_dir = 'data/food/'\n",
        "    image_datasets, dataloaders = read_data(data_dir)\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"valid\"]}\n",
        "    class_names = image_datasets[\"train\"].classes\n",
        "    inputs, classes = next(iter(dataloaders[\"valid\"]))\n",
        "    plt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample valid Images\")\n",
        "    plt.imshow(np.transpose(utils.make_grid(inputs, padding=1, normalize=True),(1, 2, 0)));"
      ],
      "id": "606d3d05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Dataset statistics\n",
        "\\\n",
        "\n",
        "Here is the stat of our toy dataset. \n"
      ],
      "id": "da02c88a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    print(f\"Classes: {image_datasets['train'].classes}\")\n",
        "    print(f\"Class count: {image_datasets['train'].targets.count(0)}, {image_datasets['train'].targets.count(1)}, {image_datasets['train'].targets.count(2)}\")\n",
        "    print(f\"Samples:\", len(image_datasets[\"train\"]))\n",
        "    print(f\"First sample: {image_datasets['train'].samples[0]}\")"
      ],
      "id": "7c759513",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-trained models to extract features \n",
        "\\\n",
        "\n",
        "- Now for each image in our dataset, we'll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.  \n"
      ],
      "id": "f0d57e9d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "densenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\n",
        "densenet.classifier = nn.Identity()  # remove that last \"classification\" layer\n",
        "Z_train, y_train, Z_valid, y_valid = get_features(\n",
        "    densenet, dataloaders[\"train\"], dataloaders[\"valid\"]\n",
        ")"
      ],
      "id": "4ec11cb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shape of the feature vector\n",
        "\\\n",
        "\n",
        "- Now we have extracted feature vectors for all examples. What's the shape of these features?\n"
      ],
      "id": "bf469471"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Z_train.shape"
      ],
      "id": "199c5fd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.  \n",
        "\n",
        "![](img/densenet-architecture.png)\n",
        "\n",
        "[Source](https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a)\n",
        "\n",
        "## A feature vector given by densenet \n",
        "\\ \n",
        "\n",
        "- Let's examine the feature vectors. \n"
      ],
      "id": "c9d0e91f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(Z_train).head()"
      ],
      "id": "4dd95829",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The features are hard to interpret but they have some important information about the images which can be useful for classification.  \n",
        "\n",
        "## Logistic regression with the extracted features \n",
        "\\\n",
        "\n",
        "- Let's try out logistic regression on these extracted features. \n"
      ],
      "id": "8768aec9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))\n",
        "pipe.fit(Z_train, y_train)\n",
        "print(\"Training score: \", pipe.score(Z_train, y_train))"
      ],
      "id": "a17bc21f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipe.score(Z_valid, y_valid)\n",
        "print(\"Validation score: \", pipe.score(Z_valid, y_valid))"
      ],
      "id": "983b63be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This is great accuracy for so little data and little effort!!!\n",
        "\n",
        "\n",
        "## Sample predictions\n",
        "\\\n",
        "\n",
        "Let's examine some sample predictions on the validation set.  \n",
        "\n",
        "::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n"
      ],
      "id": "a2223799"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show predictions for 25 images in the validation set (5 rows of 5 images)\n",
        "show_predictions(pipe, Z_valid, y_valid, dataloaders['valid'], class_names, num_images=40)"
      ],
      "id": "b86e8b46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Object detection \n",
        "\\\n",
        "\n",
        "- Another useful task and tool to know is object detection using YOLO model. \n",
        "- Let's identify objects in a sample image using a pretrained model called YOLO8. \n",
        "- List the objects present in this image.\n",
        "\n",
        "![](data/yolo_test/3356700488_183566145b.jpg)\n",
        "\n",
        "## Object detection using [YOLO](https://docs.ultralytics.com/)\n",
        "\\\n",
        "\n",
        "Let's try this out using a pre-trained model. \n"
      ],
      "id": "168331e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from ultralytics import YOLO\n",
        "model = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8n model\n",
        "\n",
        "yolo_input = \"data/yolo_test/3356700488_183566145b.jpg\"\n",
        "yolo_result = \"data/yolo_result.jpg\"\n",
        "# Run batched inference on a list of images\n",
        "result = model(yolo_input)  # return a list of Results objects\n",
        "result[0].save(filename=yolo_result)"
      ],
      "id": "74ad6ac0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Object detection output \n",
        "\\\n",
        "\n",
        "::: {.scroll-container style=\"overflow-y: scroll; height: 400px;\"}\n"
      ],
      "id": "c4c3e4e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Load the images\n",
        "input_img = mpimg.imread(yolo_input)\n",
        "result_img = mpimg.imread(yolo_result)\n",
        "\n",
        "# Create a figure to display the images side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Display the first image\n",
        "axes[0].imshow(input_img)\n",
        "axes[0].axis('off')  # Hide the axes\n",
        "axes[0].set_title('Original Image')\n",
        "\n",
        "# Display the second image\n",
        "axes[1].imshow(result_img)\n",
        "axes[1].axis('off')  # Hide the axes\n",
        "axes[1].set_title('Result Image')\n",
        "\n",
        "# Show the images\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "86d71935",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Summary \n",
        "\\\n",
        "\n",
        "- Neural networks are a flexible class of models.\n",
        "  - They are particular powerful for structured input like images, videos, audio, etc.\n",
        "  - They can be challenging to train and often require significant computational resources.\n",
        "- The good news is we can use pre-trained neural networks.\n",
        "  - This saves us a huge amount of time/cost/effort/resources.\n",
        "  - We can use these pre-trained networks directly or use them as feature transformers. "
      ],
      "id": "a27e9b78"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-cpsc330-24-py",
      "language": "python",
      "display_name": "conda-env-cpsc330-24-py",
      "path": "/Users/firasm/Library/Jupyter/kernels/conda-env-cpsc330-24-py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}