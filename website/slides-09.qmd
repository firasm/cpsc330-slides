---
title: "CPSC 330 Lecture 9: Classification Metrics" 
author: "Firas Moosvi (Slides adapted from Varada Kolhatkar)"
description: "Metrics for classification"
description-short: "confusion metrics, precision, recall, f1-score, PR curves, AP score, ROC curve, ROC AUC, class imbalance" 
format:
  revealjs:
    embed-resources: true
    slide-number: true
    smaller: true
    center: true
    logo: img/UBC-CS-logo.png
    resources:
      - data/
      - img/  
---

## Announcements 

- Important information about midterm 1 released on this [Piazza post](https://piazza.com/class/m01ukubppof625/post/249)
- Practice Midterm will be released today!
    - We hope to give you a sense of the exam format, types of questions, and to get you more familiar with the PrairieLearn interface. They do NOT encompass all the topics we've discussed nor reflect the actual number of questions on the actual midterm.
    - It will have a limit of 50-mins so you can get used to how the system will behave in the CBTF **BUT** the practice midterm has **NOT** been designed to be completed in 50 minutes!
    - You will be able to do the practice midterm multiple times for practice (but the practice MT questions are not randomized)
    - As promised, documentation for `numpy`, `pandas`, `scipy`, and `sklearn` is provided to you as the first question.
- HW4 has been released. Due next week Monday. 
- HW5 will be released next week Tuesday. It's a project-type assignment and you get till Oct 28th to work on it.


```{python}
import os
import sys
import pandas as pd 
import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from scipy.stats import expon, lognorm, loguniform, randint, uniform, norm, randint
%matplotlib inline
import mglearn
DATA_DIR = 'data/' 
```

## ML workflow 
![](img/ml-workflow.png)

## Classification Metrics

At the end of last class we talked about some of the problems with "accuracy", and we brainstormed some possible alternatives, and [saw that there are tonnes of options](https://en.wikipedia.org/wiki/Precision_and_recall#Definition).

Today, let's sift through the noise and develop some intuition about **why** we need classification metrics, and **how** some of them are used.

## Example from StatQuest!

Let's first walk through this [example through StatQuest](https://www.youtube.com/watch?v=4jRBRDbJemM) with obese mice and classifying them using Logistic Regression:

![](img/obese_mice)

[Source: StatQuest](https://www.youtube.com/watch?v=4jRBRDbJemM)

## Activity 1: Create Confusion Matrix

![](img/confusion_matrix.png)

[Source: StatQuest](https://www.youtube.com/watch?v=4jRBRDbJemM)

## Activity 2: Calculate Precision, Recall, Specificity

- Recall (aka Sensitivity in biomedical literature)
    - TP/(TP+FN)

- Precision
    - TP/(TP+FP)

- Specificity
    - TN/(TN+FP)

![](img/calculate_Sen_Spe.png)

## Break!

Let's take a 10 minute break.

## Confusion matrix questions 

Imagine a spam filter model where emails classified as spam are labeled 1 and non-spam emails are labeled 0. If a spam email is incorrectly classified as non-spam, what is this error called?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In an intrusion detection system, intrusions are identified as 1 and non-intrusive activities as 0. If the system fails to identify an actual intrusion, wrongly categorizing it as non-intrusive, what is this type of error called?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## Confusion matrix questions

In a medical test for a disease, diseased states are labeled as 1 and healthy states as 0. If a healthy patient is incorrectly diagnosed with the disease, what is this error known as?

- (A) A false positive
- (B) A true positive
- (C) A false negative
- (D) A true negative

## iClicker Exercise 9.1

**iClicker cloud join link: [https://join.iclicker.com/VYFJ](https://join.iclicker.com/VYFJ)**

**Select all of the following statements which are TRUE.**

- (A) In medical diagnosis, false positives are more damaging than false negatives (assume "positive" means the person has a disease, "negative" means they don't).
- (B) In spam classification, false positives are more damaging than false negatives (assume "positive" means the email is spam, "negative" means they it's not).
- (C) If method A gets a higher accuracy than method B, that means its precision is also higher.
- (D) If method A gets a higher accuracy than method B, that means its recall is also higher.

## Counter examples

Method A - higher accuracy but lower precision

| Negative | Positive
| -------- |:-------------:|
| 90      | 5|
| 5      | 0|

Method B - lower accuracy but higher precision

| Negative | Positive
| -------- |:-------------:|
| 80      | 15|
| 0      | 5|


## Thresholding Exercise 9.2

**iClicker cloud join link: [https://join.iclicker.com/VYFJ](https://join.iclicker.com/VYFJ)**

**Select all of the following statements which are TRUE.**

- (A) If we increase the classification threshold, both true and false positives are likely to decrease.
- (B) If we increase the classification threshold, both true and false negatives are likely to decrease.
- (C) Lowering the classification threshold generally increases the modelâ€™s recall.  
- (D) Raising the classification threshold can improve the precision of the model if it effectively reduces the number of false positives without significantly affecting true positives.


## ROC AUC questions

Consider the points A, B, and C in the following diagram, each representing a threshold. Which threshold would you pick in each scenario?

:::: {.columns}

:::{.column width="50%"}
![](img/auc_abc)
:::

:::{.column width="50%"}

- (A) If false positives (false alarms) are highly costly
- (B) If false positives are cheap and false negatives (missed true positives) highly costly
- (C) If the costs are roughly equivalent
:::
::::

[Source](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)